{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BGBkRUir9f7g",
    "is_executing": true,
    "outputId": "ad00051b-6f8c-4aaa-a806-a3963ce80bdb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "base_path = \"dataset\"\n",
    "\n",
    "if not os.path.isdir(base_path):\n",
    "  os.makedirs(base_path)\n",
    "\n",
    "\n",
    "dataset_tar_path = os.path.join(base_path,\"vgg_face_dataset.tar.gz\")\n",
    "\n",
    "if not os.path.isfile(dataset_tar_path):\n",
    "  vgg_face_dataset_url = \"http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\"\n",
    "  \n",
    "  with request.urlopen(vgg_face_dataset_url) as r, open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\"), 'wb') as f:\n",
    "    f.write(r.read())\n",
    "\n",
    "  with tarfile.open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\")) as f:\n",
    "    f.extractall(os.path.join(base_path))\n",
    "\n",
    "# check if the haarcascade file exists\n",
    "if not os.path.isfile(os.path.join(base_path, \"haarcascade_frontalface_default.xml\")):\n",
    "  \n",
    "  trained_haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "  with request.urlopen(trained_haarcascade_url) as r, open(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"), 'wb') as f:\n",
    "      f.write(r.read())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image,labels=np.array([])):\n",
    "    for img in image:\n",
    "        # plt.figure(figsize=(1, 1))\n",
    "        # plt.subplot(1, len(images), i + 1)\n",
    "        try:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def get_celeb_txt_file(celeb_name):\n",
    "    return [subject for subject in sorted(os.listdir(os.path.join(base_path, \"vgg_face_dataset\", \"files\"))) if subject.startswith(celeb_name) and subject.endswith(\".txt\")]\n",
    "\n",
    "\n",
    "def get_images(subject, nb_images):\n",
    "    with open(os.path.join(base_path, \"vgg_face_dataset\", \"files\", subject), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    images_ = []\n",
    "    for line in lines:\n",
    "        url = line[line.find(\"http://\"): line.find(\".jpg\") + 4]\n",
    "        try:\n",
    "            res = request.urlopen(url,timeout=5)\n",
    "            img = np.asarray(bytearray(res.read()), dtype=\"uint8\")\n",
    "            img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "            images_.append(img)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if len(images_) == nb_images:\n",
    "            break\n",
    "\n",
    "    print(\"Number of images found: \", len(images_))\n",
    "    return images_\n",
    "\n",
    "\n",
    "def save_images_to_path(images, folder_path, person_name):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        try:\n",
    "            # Create a unique filename for each image\n",
    "            image_path = os.path.join(folder_path, f\"{person_name}_{i}.jpg\")\n",
    "\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis(\"off\")\n",
    "            plt.savefig(image_path)\n",
    "            plt.close()  # Close the current figure to avoid memory issues\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {i}: {e}\")\n",
    "            \n",
    "\n",
    "def download_and_save_person_images(person_name, celeb_txt, images_folder, nb_images=20):\n",
    "    person_folder = os.path.join(images_folder, person_name)\n",
    "    \n",
    "    if not os.path.isdir(person_folder):\n",
    "        training_folder = os.path.join(person_folder, \"training\")\n",
    "        os.makedirs(training_folder, exist_ok=True)\n",
    "        \n",
    "        person_images = get_images(celeb_txt, nb_images)\n",
    "        save_images_to_path(person_images, training_folder, person_name)\n",
    "        return person_images\n",
    "    \n",
    "    # go though the training folder and return the images\n",
    "    images = []\n",
    "    for image in os.listdir(os.path.join(person_folder, \"training\")):\n",
    "        img = cv2.imread(os.path.join(person_folder, \"training\", person_name, image))\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "def create_testing_sets():\n",
    "    for person in os.listdir(images_folder):\n",
    "        person_folder = os.path.join(images_folder, person)\n",
    "        training_folder = os.path.join(person_folder, \"training\")\n",
    "        test_folder = os.path.join(person_folder, \"testing\")\n",
    "        \n",
    "        if not os.path.isdir(test_folder):\n",
    "            os.makedirs(test_folder, exist_ok=True)\n",
    "            \n",
    "            for image in random.sample(os.listdir(training_folder), nb_test_images):\n",
    "                image_path = os.path.join(training_folder, image)\n",
    "                os.rename(image_path, os.path.join(test_folder, image))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ryan Reynolds:**\n",
    "\n",
    "*Male:* Variations in facial expressions, hairstyles, however his scruffy 5 o'clock shadow is pretty much always a part of his appearance, his larger than average forehead is also a key focus point, his point upside down triangle head also made him a person of interest. \n",
    "\n",
    "**Ryan Phillippe:**\n",
    "\n",
    "*Male:* American actor with a unique and recognizable facial structure. Explore images that highlight different expressions and angles to capture his distinct appearance.\n",
    "\n",
    "**Regina Hall:**\n",
    "\n",
    "*Female:* African American actress with a dynamic and engaging presence. Emphasize diversity in hairstyles, makeup, and expressions to showcase the versatility of her appearance.\n",
    "\n",
    "**Tamara Taylor:**\n",
    "\n",
    "*Female:* Canadian actress known for her captivating looks. Highlight different aspects of her appearance, including expressions and roles that showcase her versatility.\n",
    "\n",
    "**Ryan Reynolds and Ryan Phillippe (Persons A and C):**\n",
    "\n",
    "Shared Characteristics: Both are male, have a similar facial structure, and share some genetic features\n",
    "\n",
    "**Regina Hall and Tamara Taylor (Persons B and D):**\n",
    "\n",
    "Shared Characteristics: Both are light skinned black women, with similar facial features and relatively similar hair styles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = os.path.join(\"images\") \n",
    "\n",
    "ryan_reynolds = get_celeb_txt_file(\"Ryan_Reynolds\")\n",
    "regina_hall = get_celeb_txt_file(\"Regina_Hall\")\n",
    "ryan_phillippe = get_celeb_txt_file(\"Ryan_Phillippe\")\n",
    "tamara_taylor = get_celeb_txt_file(\"Tamara_Taylor\")\n",
    "\n",
    "\n",
    "person_a_images = []\n",
    "person_b_images = []\n",
    "person_c_images = []\n",
    "person_d_images = []\n",
    "\n",
    "\n",
    "nb_images = 40\n",
    "nb_test_images = 10\n",
    "\n",
    "person_a_images = download_and_save_person_images(\"person_a\", ryan_reynolds[0], images_folder, nb_images)\n",
    "person_b_images = download_and_save_person_images(\"person_b\", regina_hall[0], images_folder, nb_images)\n",
    "person_c_images = download_and_save_person_images(\"person_c\", ryan_phillippe[0], images_folder, nb_images)\n",
    "person_d_images = download_and_save_person_images(\"person_d\", tamara_taylor[0], images_folder, nb_images)\n",
    "\n",
    "\n",
    "create_testing_sets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faces(images):\n",
    "    faceCascade = cv2.CascadeClassifier(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"))\n",
    "    faces = []\n",
    "    for img in images:\n",
    "        img_ = img.copy()\n",
    "        img_gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
    "        faces_ = faceCascade.detectMultiScale(\n",
    "            img_gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        faces.append(faces_)\n",
    "        \n",
    "    print(\"Found {} face(s)!\".format(len(faces)))\n",
    "    return faces\n",
    "\n",
    "\n",
    "def get_image_file_paths(base_path):\n",
    "    training_image_paths = []\n",
    "    testing_image_paths = []\n",
    "\n",
    "    training_folder = os.path.join(base_path, \"training\")\n",
    "    testing_folder = os.path.join(base_path, \"testing\")\n",
    "    \n",
    "    for image in os.listdir(training_folder):\n",
    "        training_image_paths.append(os.path.join(training_folder, image).replace(\"\\\\\", \"/\"))\n",
    "        \n",
    "    for image in os.listdir(testing_folder):\n",
    "        testing_image_paths.append(os.path.join(testing_folder, image).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "    return {\"training\": training_image_paths, \"testing\": testing_image_paths}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_temp_training_testing_folder(file_paths):\n",
    "    if not os.path.isdir(tmp_train_folder):\n",
    "        os.mkdir(tmp_train_folder)\n",
    "    \n",
    "    if not os.path.isdir(tmp_test_folder):\n",
    "        os.mkdir(tmp_test_folder)\n",
    "\n",
    "    for image_path in file_paths[\"training\"]:\n",
    "        person_name = image_path.split(\"/\")[1]\n",
    "        destination_folder = os.path.join(tmp_train_folder, person_name)\n",
    "        \n",
    "        if not os.path.isdir(destination_folder):\n",
    "            os.mkdir(destination_folder)\n",
    "\n",
    "        if not os.path.isfile(os.path.join(tmp_test_folder, person_name, os.path.basename(image_path))):\n",
    "            copyfile(image_path, os.path.join(destination_folder, os.path.basename(image_path)))\n",
    "    \n",
    "    for image_path in file_paths[\"testing\"]:\n",
    "        person_name = image_path.split(\"/\")[1]\n",
    "        destination_folder = os.path.join(tmp_test_folder, person_name)\n",
    "        \n",
    "        if not os.path.isdir(destination_folder):\n",
    "            os.mkdir(destination_folder)\n",
    "\n",
    "        if not os.path.isfile(os.path.join(tmp_train_folder, person_name, os.path.basename(image_path))):\n",
    "            copyfile(image_path, os.path.join(destination_folder, os.path.basename(image_path)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "person_a_images_file_paths = get_image_file_paths(\"images/person_a/\")\n",
    "person_b_images_file_paths = get_image_file_paths(\"images/person_b/\")\n",
    "person_c_images_file_paths = get_image_file_paths(\"images/person_c/\")\n",
    "person_d_images_file_paths = get_image_file_paths(\"images/person_d/\")\n",
    "\n",
    "tmp_train_folder = os.path.join(base_path, \"tmp_train\");\n",
    "tmp_test_folder = os.path.join(base_path, \"tmp_test\");\n",
    "\n",
    "all_image_file_paths = {\n",
    "    'training': person_a_images_file_paths['training'] + person_b_images_file_paths['training'] + person_c_images_file_paths['training'] + person_d_images_file_paths['training'],\n",
    "    'testing': person_a_images_file_paths['testing'] + person_b_images_file_paths['testing' ] + person_c_images_file_paths['testing'] + person_d_images_file_paths['testing']\n",
    "}\n",
    "\n",
    "\n",
    "initialize_temp_training_testing_folder(all_image_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Load VGG19 base model\n",
    "    base_model = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(224, 224, 3),\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=\"softmax\",\n",
    "    )\n",
    "\n",
    "    # Freeze the layers of the base models\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create a new model with additional layers on top of the base_model\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(2048, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(2048, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(4, activation=\"softmax\")(x) \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 109 images belonging to 4 classes.\n",
      "Found 40 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:From s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3453 - accuracy: 0.7156WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 1.3453 - accuracy: 0.7156\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0360 - accuracy: 1.0000\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.7706WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.6364 - accuracy: 0.7706\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.8073WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.4695 - accuracy: 0.8073\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9541WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0779 - accuracy: 0.9541\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9817WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0518 - accuracy: 0.9817\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9266WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.1645 - accuracy: 0.9266\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.8991WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.2344 - accuracy: 0.8991\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9266WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.2003 - accuracy: 0.9266\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9725WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.1176 - accuracy: 0.9725\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0459 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0415 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9817WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0727 - accuracy: 0.9817\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9633WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.1047 - accuracy: 0.9633\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9725WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.1036 - accuracy: 0.9725\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9725WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0973 - accuracy: 0.9725\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9908WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0567 - accuracy: 0.9908\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0384 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9908WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0347 - accuracy: 0.9908\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9725WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0596 - accuracy: 0.9725\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9908WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0537 - accuracy: 0.9908\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9908WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0579 - accuracy: 0.9908\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9908WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.0296 - accuracy: 0.9908\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9908WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0230 - accuracy: 0.9908\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              1050624   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 8196      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19969860 (76.18 MB)\n",
      "Trainable params: 5255172 (20.05 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Training data generator with augmentation\n",
    "trData = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Testing data generator (without augmentation)\n",
    "tsData = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "target_size = (224, 224)\n",
    "class_mode = 'categorical'\n",
    "epochs = 40\n",
    "learning_rate=0.001\n",
    "\n",
    "train_data = trData.flow_from_directory(\n",
    "    directory=tmp_train_folder,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode, \n",
    ")\n",
    "\n",
    "test_data = tsData.flow_from_directory(\n",
    "    directory=tmp_test_folder,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./models/checkpoint_vgg16.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "callbacks = [checkpoint, early_stop]\n",
    "\n",
    "if os.path.isfile(\"./models/vgg16_custom.h5\"):\n",
    "    # model = tf.keras.models.load_model(\"./models/vgg16_custom.h5\")\n",
    "    print(\"Loading model from file\")\n",
    "else:\n",
    "    model = create_model()\n",
    "\n",
    "class_labels = list(test_data.class_indices.keys())\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "steps_per_epoch = len(train_data)\n",
    "\n",
    "\n",
    "# create validation data by splitting the training data in half\n",
    "# test_data, val_dat = train_test_split(test_data, test_size=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, epochs=epochs, callbacks=callbacks, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"./models/vgg16_custom.h5\")\n",
    "# delete the checkpoint file\n",
    "if os.path.isfile(\"./models/checkpoint_vgg16.h5\"):\n",
    "    os.remove(\"./models/checkpoint_vgg16.h5\")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 4s 805ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate The Model\n",
    "\n",
    "\n",
    "test_batch = next(test_data)\n",
    "predictions = model.predict(test_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_predictions(class_labels,predictions):\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "        true_label =class_labels[np.argmax(test_batch[1][idx])]\n",
    "        plt.imshow(test_batch[0][idx])\n",
    "        plt.title(f\"Predicted: {predicted_label} - Actual: {true_label}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment If you want to display predictions\n",
    "# display_predictions(class_labels,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions_array = np.array(predictions)\n",
    "true_labels_array = np.array(test_batch[1])\n",
    "\n",
    "predicted_classes = np.argmax(predictions_array, axis=1)\n",
    "true_classes = np.argmax(true_labels_array, axis=1)\n",
    "\n",
    "predicted_labels = np.array(class_labels)[predicted_classes]\n",
    "true_labels = np.array(class_labels)[true_classes]\n",
    "\n",
    "\n",
    "cm = confusion_matrix(predicted_labels, true_labels, labels = class_labels)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "# Add labels to the axes\n",
    "plt.xlabel('Predicted Person')\n",
    "plt.ylabel('True Person')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the folder: 5\n",
      "Image files: ['208580.jpeg', 'deacon-Phillippe-ryan-Phillippe-main-lc-220803-e72471.jpeg', 'image-6.jpeg', 'Tamara-Taylor-Main-2023.jpeg', 'teyana-taylor-abs-11.jpeg']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (224,224,3) into shape (224,224)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[338], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     predicted_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(class_labels)[predicted_classes]\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_labels\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmake_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[338], line 22\u001b[0m, in \u001b[0;36mmake_predictions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(images)\n\u001b[0;32m     25\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (224,224,3) into shape (224,224)"
     ]
    }
   ],
   "source": [
    "unknown_pics_folder = os.path.join(base_path, \"pics\")\n",
    "\n",
    "# Print all the images in the unknown_pics_folder\n",
    "image_files = os.listdir(unknown_pics_folder)\n",
    "print(f\"Number of images in the folder: {len(image_files)}\")\n",
    "print(\"Image files:\", image_files)\n",
    "\n",
    "def make_predictions():\n",
    "    images = []\n",
    "    common_shape = (target_size[0], target_size[1], 3)  # Assuming 3 channels for color images\n",
    "\n",
    "    for img_file_name in image_files:\n",
    "        pic_path = os.path.join(unknown_pics_folder, img_file_name)\n",
    "        try:\n",
    "            img_array = imread(pic_path)\n",
    "            img_array = cv2.resize(img_array, (target_size[0], target_size[1]))\n",
    "            # img_array = img_array / 255.0\n",
    "            images.append(img_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file_name}: {e}\")\n",
    "\n",
    "    images = np.array(images, dtype=\"object\")\n",
    "    \n",
    "    predictions = model.predict(images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    predicted_labels = np.array(class_labels)[predicted_classes]\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "make_predictions()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
