{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BGBkRUir9f7g",
    "is_executing": true,
    "outputId": "ad00051b-6f8c-4aaa-a806-a3963ce80bdb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "\n",
    "\n",
    "base_path = \"dataset\"\n",
    "\n",
    "if not os.path.isdir(base_path):\n",
    "  os.makedirs(base_path)\n",
    "\n",
    "\n",
    "dataset_tar_path = os.path.join(base_path,\"vgg_face_dataset.tar.gz\")\n",
    "\n",
    "if not os.path.isfile(dataset_tar_path):\n",
    "  vgg_face_dataset_url = \"http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\"\n",
    "  \n",
    "  with request.urlopen(vgg_face_dataset_url) as r, open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\"), 'wb') as f:\n",
    "    f.write(r.read())\n",
    "\n",
    "  with tarfile.open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\")) as f:\n",
    "    f.extractall(os.path.join(base_path))\n",
    "\n",
    "# check if the haarcascade file exists\n",
    "if not os.path.isfile(os.path.join(base_path, \"haarcascade_frontalface_default.xml\")):\n",
    "  \n",
    "  trained_haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "  with request.urlopen(trained_haarcascade_url) as r, open(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"), 'wb') as f:\n",
    "      f.write(r.read())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image):\n",
    "    for img in image:\n",
    "        # plt.figure(figsize=(1, 1))\n",
    "        #   plt.subplot(1, len(images), i + 1)\n",
    "        try:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def get_celeb_txt_file(celeb_name):\n",
    "    return [subject for subject in sorted(os.listdir(os.path.join(base_path, \"vgg_face_dataset\", \"files\"))) if subject.startswith(celeb_name) and subject.endswith(\".txt\")]\n",
    "\n",
    "\n",
    "def get_images(subject, nb_images):\n",
    "    with open(os.path.join(base_path, \"vgg_face_dataset\", \"files\", subject), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    images_ = []\n",
    "    for line in lines:\n",
    "        url = line[line.find(\"http://\"): line.find(\".jpg\") + 4]\n",
    "        try:\n",
    "            res = request.urlopen(url)\n",
    "            img = np.asarray(bytearray(res.read()), dtype=\"uint8\")\n",
    "            img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "            images_.append(img)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if len(images_) == nb_images:\n",
    "            break\n",
    "\n",
    "    print(\"Number of images found: \", len(images_))\n",
    "    return images_\n",
    "\n",
    "\n",
    "def save_images_to_path(images, folder_path, person_name):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        try:\n",
    "            # Create a unique filename for each image\n",
    "            image_path = os.path.join(folder_path, f\"{person_name}_{i}.jpg\")\n",
    "\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis(\"off\")\n",
    "            plt.savefig(image_path)\n",
    "            plt.close()  # Close the current figure to avoid memory issues\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {i}: {e}\")\n",
    "            \n",
    "\n",
    "def download_and_save_person_images(person_name, celeb_txt, images_folder, nb_images=20):\n",
    "    person_folder = os.path.join(images_folder, person_name)\n",
    "    \n",
    "    if not os.path.isdir(person_folder):\n",
    "        training_folder = os.path.join(person_folder, \"training\")\n",
    "        os.makedirs(training_folder, exist_ok=True)\n",
    "        \n",
    "        person_images = get_images(celeb_txt, nb_images)\n",
    "        save_images_to_path(person_images, training_folder, person_name)\n",
    "        return person_images\n",
    "    \n",
    "    # go though the training folder and return the images\n",
    "    images = []\n",
    "    for image in os.listdir(os.path.join(person_folder, \"training\")):\n",
    "        img = cv2.imread(os.path.join(person_folder, \"training\", person_name, image))\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "def create_testing_sets():\n",
    "    for person in os.listdir(images_folder):\n",
    "        person_folder = os.path.join(images_folder, person)\n",
    "        training_folder = os.path.join(person_folder, \"training\")\n",
    "        test_folder = os.path.join(person_folder, \"testing\")\n",
    "        \n",
    "        if not os.path.isdir(test_folder):\n",
    "            os.makedirs(test_folder, exist_ok=True)\n",
    "            \n",
    "            for image in random.sample(os.listdir(training_folder), nb_test_images):\n",
    "                image_path = os.path.join(training_folder, image)\n",
    "                os.rename(image_path, os.path.join(test_folder, image))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ryan Reynolds:**\n",
    "\n",
    "*Male:* Variations in facial expressions, hairstyles, however his scruffy 5 o'clock shadow is pretty much always a part of his appearance, his larger than average forehead is also a key focus point, his point upside down triangle head also made him a person of interest. \n",
    "\n",
    "**Ryan Phillippe:**\n",
    "\n",
    "*Male:* American actor with a unique and recognizable facial structure. Explore images that highlight different expressions and angles to capture his distinct appearance.\n",
    "\n",
    "**Regina Hall:**\n",
    "\n",
    "*Female:* African American actress with a dynamic and engaging presence. Emphasize diversity in hairstyles, makeup, and expressions to showcase the versatility of her appearance.\n",
    "\n",
    "**Tamara Taylor:**\n",
    "\n",
    "*Female:* Canadian actress known for her captivating looks. Highlight different aspects of her appearance, including expressions and roles that showcase her versatility.\n",
    "\n",
    "**Ryan Reynolds and Ryan Phillippe (Persons A and C):**\n",
    "\n",
    "Shared Characteristics: Both are male, have a similar facial structure, and share some genetic features\n",
    "\n",
    "**Regina Hall and Tamara Taylor (Persons B and D):**\n",
    "\n",
    "Shared Characteristics: Both are light skinned black women, with similar facial features and relatively similar hair styles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = os.path.join(\"images\") \n",
    "\n",
    "ryan_reynolds = get_celeb_txt_file(\"Ryan_Reynolds\")\n",
    "regina_hall = get_celeb_txt_file(\"Regina_Hall\")\n",
    "ryan_phillippe = get_celeb_txt_file(\"Ryan_Phillippe\")\n",
    "tamara_taylor = get_celeb_txt_file(\"Tamara_Taylor\")\n",
    "\n",
    "\n",
    "person_a_images = []\n",
    "person_b_images = []\n",
    "person_c_images = []\n",
    "person_d_images = []\n",
    "\n",
    "\n",
    "nb_images = 40\n",
    "nb_test_images = 10\n",
    "\n",
    "person_a_images = download_and_save_person_images(\"person_a\", ryan_reynolds[0], images_folder, nb_images)\n",
    "person_b_images = download_and_save_person_images(\"person_b\", regina_hall[0], images_folder, nb_images)\n",
    "person_c_images = download_and_save_person_images(\"person_c\", ryan_phillippe[0], images_folder, nb_images)\n",
    "person_d_images = download_and_save_person_images(\"person_d\", tamara_taylor[0], images_folder, nb_images)\n",
    "\n",
    "\n",
    "create_testing_sets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_images(person_a_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faces(images):\n",
    "    faceCascade = cv2.CascadeClassifier(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"))\n",
    "    faces = []\n",
    "    for img in images:\n",
    "        img_ = img.copy()\n",
    "        img_gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
    "        faces_ = faceCascade.detectMultiScale(\n",
    "            img_gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        faces.append(faces_)\n",
    "        \n",
    "    print(\"Found {} face(s)!\".format(len(faces)))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_file_paths(base_path):\n",
    "    training_image_paths = []\n",
    "    testing_image_paths = []\n",
    "\n",
    "    training_folder = os.path.join(base_path, \"training\")\n",
    "    testing_folder = os.path.join(base_path, \"testing\")\n",
    "    \n",
    "    for image in os.listdir(training_folder):\n",
    "        training_image_paths.append(os.path.join(training_folder, image).replace(\"\\\\\", \"/\"))\n",
    "        \n",
    "    for image in os.listdir(testing_folder):\n",
    "        testing_image_paths.append(os.path.join(testing_folder, image).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "    return {\"training\": training_image_paths, \"testing\": testing_image_paths}\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "person_a_images_base_path = \"images/person_a/\"\n",
    "person_b_images_base_path = \"images/person_b/\"\n",
    "person_c_images_base_path = \"images/person_c/\"\n",
    "person_d_images_base_path = \"images/person_d/\"\n",
    "\n",
    "# def extract_features(image_paths, label):\n",
    "#     X = []\n",
    "#     y = []\n",
    "\n",
    "#     for img_path in image_paths:\n",
    "#         try:\n",
    "#             img = image.load_img(img_path, target_size=(224, 224))\n",
    "#             img_array = image.img_to_array(img)\n",
    "#             img_array = preprocess_input(img_array)\n",
    "#             features = base_model.predict(img_array.reshape(1, 224, 224, 3))\n",
    "#             X.append(features.flatten())\n",
    "#             y.append(label)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {e}\")\n",
    "#     return X, y\n",
    "# person_a_features = extract_features(get_image_file_paths(person_a_images_base_path, nb_images), \"person_a\")\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(person_a_features['X'], person_a_features['y'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': ['images/person_a/training/person_a_0.jpg', 'images/person_a/training/person_a_1.jpg', 'images/person_a/training/person_a_10.jpg', 'images/person_a/training/person_a_11.jpg', 'images/person_a/training/person_a_12.jpg', 'images/person_a/training/person_a_13.jpg', 'images/person_a/training/person_a_15.jpg', 'images/person_a/training/person_a_19.jpg', 'images/person_a/training/person_a_2.jpg', 'images/person_a/training/person_a_20.jpg', 'images/person_a/training/person_a_22.jpg', 'images/person_a/training/person_a_23.jpg', 'images/person_a/training/person_a_24.jpg', 'images/person_a/training/person_a_26.jpg', 'images/person_a/training/person_a_27.jpg', 'images/person_a/training/person_a_28.jpg', 'images/person_a/training/person_a_3.jpg', 'images/person_a/training/person_a_30.jpg', 'images/person_a/training/person_a_32.jpg', 'images/person_a/training/person_a_34.jpg', 'images/person_a/training/person_a_35.jpg', 'images/person_a/training/person_a_36.jpg', 'images/person_a/training/person_a_37.jpg', 'images/person_a/training/person_a_38.jpg', 'images/person_a/training/person_a_39.jpg', 'images/person_a/training/person_a_4.jpg', 'images/person_a/training/person_a_6.jpg', 'images/person_a/training/person_a_8.jpg', 'images/person_a/training/person_a_9.jpg', 'images/person_b/training/person_b_1.jpg', 'images/person_b/training/person_b_14.jpg', 'images/person_b/training/person_b_15.jpg', 'images/person_b/training/person_b_16.jpg', 'images/person_b/training/person_b_17.jpg', 'images/person_b/training/person_b_18.jpg', 'images/person_b/training/person_b_2.jpg', 'images/person_b/training/person_b_20.jpg', 'images/person_b/training/person_b_21.jpg', 'images/person_b/training/person_b_22.jpg', 'images/person_b/training/person_b_23.jpg', 'images/person_b/training/person_b_24.jpg', 'images/person_b/training/person_b_25.jpg', 'images/person_b/training/person_b_26.jpg', 'images/person_b/training/person_b_27.jpg', 'images/person_b/training/person_b_28.jpg', 'images/person_b/training/person_b_29.jpg', 'images/person_b/training/person_b_3.jpg', 'images/person_b/training/person_b_30.jpg', 'images/person_b/training/person_b_36.jpg', 'images/person_b/training/person_b_37.jpg', 'images/person_b/training/person_b_38.jpg', 'images/person_b/training/person_b_5.jpg', 'images/person_b/training/person_b_7.jpg', 'images/person_b/training/person_b_9.jpg', 'images/person_c/training/person_c_11.jpg', 'images/person_c/training/person_c_13.jpg', 'images/person_c/training/person_c_14.jpg', 'images/person_c/training/person_c_15.jpg', 'images/person_c/training/person_c_16.jpg', 'images/person_c/training/person_c_17.jpg', 'images/person_c/training/person_c_18.jpg', 'images/person_c/training/person_c_2.jpg', 'images/person_c/training/person_c_21.jpg', 'images/person_c/training/person_c_22.jpg', 'images/person_c/training/person_c_23.jpg', 'images/person_c/training/person_c_24.jpg', 'images/person_c/training/person_c_26.jpg', 'images/person_c/training/person_c_27.jpg', 'images/person_c/training/person_c_28.jpg', 'images/person_c/training/person_c_29.jpg', 'images/person_c/training/person_c_3.jpg', 'images/person_c/training/person_c_31.jpg', 'images/person_c/training/person_c_32.jpg', 'images/person_c/training/person_c_33.jpg', 'images/person_c/training/person_c_35.jpg', 'images/person_c/training/person_c_36.jpg', 'images/person_c/training/person_c_37.jpg', 'images/person_c/training/person_c_4.jpg', 'images/person_c/training/person_c_5.jpg', 'images/person_c/training/person_c_6.jpg', 'images/person_c/training/person_c_8.jpg', 'images/person_c/training/person_c_9.jpg', 'images/person_d/training/person_d_0.jpg', 'images/person_d/training/person_d_10.jpg', 'images/person_d/training/person_d_12.jpg', 'images/person_d/training/person_d_14.jpg', 'images/person_d/training/person_d_15.jpg', 'images/person_d/training/person_d_16.jpg', 'images/person_d/training/person_d_17.jpg', 'images/person_d/training/person_d_18.jpg', 'images/person_d/training/person_d_19.jpg', 'images/person_d/training/person_d_20.jpg', 'images/person_d/training/person_d_23.jpg', 'images/person_d/training/person_d_24.jpg', 'images/person_d/training/person_d_25.jpg', 'images/person_d/training/person_d_26.jpg', 'images/person_d/training/person_d_28.jpg', 'images/person_d/training/person_d_29.jpg', 'images/person_d/training/person_d_31.jpg', 'images/person_d/training/person_d_33.jpg', 'images/person_d/training/person_d_35.jpg', 'images/person_d/training/person_d_36.jpg', 'images/person_d/training/person_d_37.jpg', 'images/person_d/training/person_d_38.jpg', 'images/person_d/training/person_d_39.jpg', 'images/person_d/training/person_d_6.jpg', 'images/person_d/training/person_d_7.jpg', 'images/person_d/training/person_d_8.jpg', 'images/person_d/training/person_d_9.jpg'], 'testing': ['images/person_a/testing/person_a_16.jpg', 'images/person_a/testing/person_a_17.jpg', 'images/person_a/testing/person_a_18.jpg', 'images/person_a/testing/person_a_21.jpg', 'images/person_a/testing/person_a_25.jpg', 'images/person_a/testing/person_a_29.jpg', 'images/person_a/testing/person_a_31.jpg', 'images/person_a/testing/person_a_33.jpg', 'images/person_a/testing/person_a_5.jpg', 'images/person_a/testing/person_a_7.jpg', 'images/person_b/testing/person_b_10.jpg', 'images/person_b/testing/person_b_11.jpg', 'images/person_b/testing/person_b_12.jpg', 'images/person_b/testing/person_b_13.jpg', 'images/person_b/testing/person_b_19.jpg', 'images/person_b/testing/person_b_31.jpg', 'images/person_b/testing/person_b_33.jpg', 'images/person_b/testing/person_b_39.jpg', 'images/person_b/testing/person_b_6.jpg', 'images/person_b/testing/person_b_8.jpg', 'images/person_c/testing/person_c_0.jpg', 'images/person_c/testing/person_c_1.jpg', 'images/person_c/testing/person_c_10.jpg', 'images/person_c/testing/person_c_12.jpg', 'images/person_c/testing/person_c_19.jpg', 'images/person_c/testing/person_c_20.jpg', 'images/person_c/testing/person_c_25.jpg', 'images/person_c/testing/person_c_38.jpg', 'images/person_c/testing/person_c_39.jpg', 'images/person_c/testing/person_c_7.jpg', 'images/person_d/testing/person_d_11.jpg', 'images/person_d/testing/person_d_13.jpg', 'images/person_d/testing/person_d_2.jpg', 'images/person_d/testing/person_d_21.jpg', 'images/person_d/testing/person_d_27.jpg', 'images/person_d/testing/person_d_3.jpg', 'images/person_d/testing/person_d_30.jpg', 'images/person_d/testing/person_d_32.jpg', 'images/person_d/testing/person_d_4.jpg', 'images/person_d/testing/person_d_5.jpg']}\n",
      "Found 109 images belonging to 4 classes.\n",
      "Found 40 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "person_a_images_file_paths = get_image_file_paths(person_a_images_base_path)\n",
    "person_b_images_file_paths = get_image_file_paths(person_b_images_base_path)\n",
    "person_c_images_file_paths = get_image_file_paths(person_c_images_base_path)\n",
    "person_d_images_file_paths = get_image_file_paths(person_d_images_base_path)\n",
    "\n",
    "tmp_train_folder = os.path.join(base_path, \"tmp_train\");\n",
    "tmp_test_folder = os.path.join(base_path, \"tmp_test\");\n",
    "\n",
    "def initialize_temp_training_testing_folder(file_paths):\n",
    "    if not os.path.isdir(tmp_train_folder):\n",
    "        os.mkdir(tmp_train_folder)\n",
    "    \n",
    "    if not os.path.isdir(tmp_test_folder):\n",
    "        os.mkdir(tmp_test_folder)\n",
    "        \n",
    "    \n",
    "    # if the file oath contains train then copy to tmp_train_folder else copy to tmp_test_folder\n",
    "    for image_path in file_paths[\"training\"]:\n",
    "        person_name = image_path.split(\"/\")[1]\n",
    "        if not os.path.isdir(os.path.join(tmp_train_folder, person_name)):\n",
    "            os.mkdir(os.path.join(tmp_train_folder, person_name))\n",
    "            destination_folder = os.path.join(tmp_train_folder, person_name)\n",
    "        copyfile(image_path, os.path.join(destination_folder, os.path.basename(image_path)))\n",
    "    \n",
    "    for image_path in file_paths[\"testing\"]:\n",
    "        person_name = image_path.split(\"/\")[1]\n",
    "        if not os.path.isdir(os.path.join(tmp_test_folder, person_name)):\n",
    "            os.mkdir(os.path.join(tmp_test_folder, person_name))\n",
    "            destination_folder = os.path.join(tmp_test_folder, person_name)\n",
    "        copyfile(image_path, os.path.join(destination_folder, os.path.basename(image_path)))\n",
    "    \n",
    "\n",
    "# merge the two dictionaries\n",
    "# Merge the two dictionaries\n",
    "all_image_file_paths = {\n",
    "    'training': person_a_images_file_paths['training'] + person_b_images_file_paths['training'] + person_c_images_file_paths['training'] + person_d_images_file_paths['training'],\n",
    "    'testing': person_a_images_file_paths['testing'] + person_b_images_file_paths['testing' ] + person_c_images_file_paths['testing'] + person_d_images_file_paths['testing']\n",
    "}\n",
    "\n",
    "# Print the combined paths\n",
    "print(all_image_file_paths)\n",
    "\n",
    "\n",
    "initialize_temp_training_testing_folder(all_image_file_paths)\n",
    "\n",
    "\n",
    "\n",
    "trData = ImageDataGenerator()\n",
    "train_data = trData.flow_from_directory(directory=tmp_train_folder, target_size=(224, 224))\n",
    "tsData = ImageDataGenerator()\n",
    "test_data = tsData.flow_from_directory(directory=tmp_test_folder, target_size=(224, 224)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johna\\AppData\\Local\\Temp\\ipykernel_8700\\4105649403.py:15: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  base_model.fit_generator(train_data, steps_per_epoch=100, epochs=10, validation_data=test_data, validation_steps=10, callbacks=callbacks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 529, in dispatch_queue\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 518, in process_one\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 424, in dispatch_shell\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 766, in execute_request\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3046, in run_cell\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3101, in _run_cell\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3488, in run_ast_nodes\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n\n  File \"C:\\Users\\johna\\AppData\\Local\\Temp\\ipykernel_8700\\4105649403.py\", line 15, in <module>\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2913, in fit_generator\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[32,1000] labels_size=[32,4]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_6653]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32ms:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\Assignment_Deep_Learning.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/Development/kdg/Data/5/d-learning-transfer-learning/Assignment_Deep_Learning.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m base_model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/Development/kdg/Data/5/d-learning-transfer-learning/Assignment_Deep_Learning.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#  Build a binary classifier\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/Development/kdg/Data/5/d-learning-transfer-learning/Assignment_Deep_Learning.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# vgg16_custom_model = Sequential()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/Development/kdg/Data/5/d-learning-transfer-learning/Assignment_Deep_Learning.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# vgg16_custom_model.add(Flatten(input_shape=(7, 7, 512)))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/Development/kdg/Data/5/d-learning-transfer-learning/Assignment_Deep_Learning.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# vgg16_custom_model.add(Dense(256, activation='relu'))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/Development/kdg/Data/5/d-learning-transfer-learning/Assignment_Deep_Learning.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# vgg16_custom_model.add(Dense(1, activation='sigmoid'))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/s%3A/Development/kdg/Data/5/d-learning-transfer-learning/Assignment_Deep_Learning.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m base_model\u001b[39m.\u001b[39;49mfit_generator(train_data, steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mtest_data, validation_steps\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/Development/kdg/Data/5/d-learning-transfer-learning/Assignment_Deep_Learning.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m base_model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mvgg16_1.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32ms:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:2913\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2901\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2902\u001b[0m \n\u001b[0;32m   2903\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2904\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2905\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2906\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2907\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2908\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2909\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2910\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2911\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2912\u001b[0m )\n\u001b[1;32m-> 2913\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   2914\u001b[0m     generator,\n\u001b[0;32m   2915\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   2916\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   2917\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2918\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   2919\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[0;32m   2920\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   2921\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[0;32m   2922\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2923\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2924\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2925\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2926\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   2927\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   2928\u001b[0m )\n",
      "File \u001b[1;32ms:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32ms:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 529, in dispatch_queue\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 518, in process_one\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 424, in dispatch_shell\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 766, in execute_request\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3046, in run_cell\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3101, in _run_cell\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3488, in run_ast_nodes\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n\n  File \"C:\\Users\\johna\\AppData\\Local\\Temp\\ipykernel_8700\\4105649403.py\", line 15, in <module>\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2913, in fit_generator\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n\n  File \"s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[32,1000] labels_size=[32,4]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_6653]"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n",
    "early_stop = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "callbacks = [checkpoint, early_stop]\n",
    "base_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#  Build a binary classifier\n",
    "# vgg16_custom_model = Sequential()\n",
    "# vgg16_custom_model.add(Flatten(input_shape=(7, 7, 512)))\n",
    "# vgg16_custom_model.add(Dense(256, activation='relu'))\n",
    "# vgg16_custom_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "base_model.fit_generator(train_data, steps_per_epoch=100, epochs=10, validation_data=test_data, validation_steps=10, callbacks=callbacks)\n",
    "base_model.save(\"vgg16_1.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile and train the model\n",
    "# model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(np.array(person_a_features[0]), np.array(person_a_features[1]), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate The Model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
