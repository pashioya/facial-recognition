{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BGBkRUir9f7g",
    "is_executing": true,
    "outputId": "ad00051b-6f8c-4aaa-a806-a3963ce80bdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "base_path = \"dataset\"\n",
    "\n",
    "if not os.path.isdir(base_path):\n",
    "  os.makedirs(base_path)\n",
    "\n",
    "\n",
    "dataset_tar_path = os.path.join(base_path,\"vgg_face_dataset.tar.gz\")\n",
    "\n",
    "if not os.path.isfile(dataset_tar_path):\n",
    "  vgg_face_dataset_url = \"http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\"\n",
    "  \n",
    "  with request.urlopen(vgg_face_dataset_url) as r, open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\"), 'wb') as f:\n",
    "    f.write(r.read())\n",
    "\n",
    "  with tarfile.open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\")) as f:\n",
    "    f.extractall(os.path.join(base_path))\n",
    "\n",
    "# check if the haarcascade file exists\n",
    "if not os.path.isfile(os.path.join(base_path, \"haarcascade_frontalface_default.xml\")):\n",
    "  \n",
    "  trained_haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "  with request.urlopen(trained_haarcascade_url) as r, open(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"), 'wb') as f:\n",
    "      f.write(r.read())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image,labels=np.array([])):\n",
    "    for img in image:\n",
    "        # plt.figure(figsize=(1, 1))\n",
    "        #   plt.subplot(1, len(images), i + 1)\n",
    "        try:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            # plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def get_celeb_txt_file(celeb_name):\n",
    "    return [subject for subject in sorted(os.listdir(os.path.join(base_path, \"vgg_face_dataset\", \"files\"))) if subject.startswith(celeb_name) and subject.endswith(\".txt\")]\n",
    "\n",
    "\n",
    "def get_images(subject, nb_images):\n",
    "    with open(os.path.join(base_path, \"vgg_face_dataset\", \"files\", subject), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    images_ = []\n",
    "    for line in lines:\n",
    "        url = line[line.find(\"http://\"): line.find(\".jpg\") + 4]\n",
    "        try:\n",
    "            res = request.urlopen(url,timeout=5)\n",
    "            img = np.asarray(bytearray(res.read()), dtype=\"uint8\")\n",
    "            img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "            images_.append(img)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if len(images_) == nb_images:\n",
    "            break\n",
    "\n",
    "    print(\"Number of images found: \", len(images_))\n",
    "    return images_\n",
    "\n",
    "\n",
    "def save_images_to_path(images, folder_path, person_name):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        try:\n",
    "            # Create a unique filename for each image\n",
    "            image_path = os.path.join(folder_path, f\"{person_name}_{i}.jpg\")\n",
    "\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis(\"off\")\n",
    "            plt.savefig(image_path)\n",
    "            plt.close()  # Close the current figure to avoid memory issues\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {i}: {e}\")\n",
    "            \n",
    "\n",
    "def download_and_save_person_images(person_name, celeb_txt, images_folder, nb_images=20):\n",
    "    person_folder = os.path.join(images_folder, person_name)\n",
    "    \n",
    "    if not os.path.isdir(person_folder):\n",
    "        training_folder = os.path.join(person_folder, \"training\")\n",
    "        os.makedirs(training_folder, exist_ok=True)\n",
    "        \n",
    "        person_images = get_images(celeb_txt, nb_images)\n",
    "        save_images_to_path(person_images, training_folder, person_name)\n",
    "        return person_images\n",
    "    \n",
    "    # go though the training folder and return the images\n",
    "    images = []\n",
    "    for image in os.listdir(os.path.join(person_folder, \"training\")):\n",
    "        img = cv2.imread(os.path.join(person_folder, \"training\", person_name, image))\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "def create_testing_sets():\n",
    "    for person in os.listdir(images_folder):\n",
    "        person_folder = os.path.join(images_folder, person)\n",
    "        training_folder = os.path.join(person_folder, \"training\")\n",
    "        test_folder = os.path.join(person_folder, \"testing\")\n",
    "        \n",
    "        if not os.path.isdir(test_folder):\n",
    "            os.makedirs(test_folder, exist_ok=True)\n",
    "            \n",
    "            for image in random.sample(os.listdir(training_folder), nb_test_images):\n",
    "                image_path = os.path.join(training_folder, image)\n",
    "                os.rename(image_path, os.path.join(test_folder, image))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ryan Reynolds:**\n",
    "\n",
    "*Male:* Variations in facial expressions, hairstyles, however his scruffy 5 o'clock shadow is pretty much always a part of his appearance, his larger than average forehead is also a key focus point, his point upside down triangle head also made him a person of interest. \n",
    "\n",
    "**Ryan Phillippe:**\n",
    "\n",
    "*Male:* American actor with a unique and recognizable facial structure. Explore images that highlight different expressions and angles to capture his distinct appearance.\n",
    "\n",
    "**Regina Hall:**\n",
    "\n",
    "*Female:* African American actress with a dynamic and engaging presence. Emphasize diversity in hairstyles, makeup, and expressions to showcase the versatility of her appearance.\n",
    "\n",
    "**Tamara Taylor:**\n",
    "\n",
    "*Female:* Canadian actress known for her captivating looks. Highlight different aspects of her appearance, including expressions and roles that showcase her versatility.\n",
    "\n",
    "**Ryan Reynolds and Ryan Phillippe (Persons A and C):**\n",
    "\n",
    "Shared Characteristics: Both are male, have a similar facial structure, and share some genetic features\n",
    "\n",
    "**Regina Hall and Tamara Taylor (Persons B and D):**\n",
    "\n",
    "Shared Characteristics: Both are light skinned black women, with similar facial features and relatively similar hair styles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = os.path.join(\"images\") \n",
    "\n",
    "ryan_reynolds = get_celeb_txt_file(\"Ryan_Reynolds\")\n",
    "regina_hall = get_celeb_txt_file(\"Regina_Hall\")\n",
    "ryan_phillippe = get_celeb_txt_file(\"Ryan_Phillippe\")\n",
    "tamara_taylor = get_celeb_txt_file(\"Tamara_Taylor\")\n",
    "\n",
    "\n",
    "person_a_images = []\n",
    "person_b_images = []\n",
    "person_c_images = []\n",
    "person_d_images = []\n",
    "\n",
    "\n",
    "nb_images = 40\n",
    "nb_test_images = 10\n",
    "\n",
    "person_a_images = download_and_save_person_images(\"person_a\", ryan_reynolds[0], images_folder, nb_images)\n",
    "person_b_images = download_and_save_person_images(\"person_b\", regina_hall[0], images_folder, nb_images)\n",
    "person_c_images = download_and_save_person_images(\"person_c\", ryan_phillippe[0], images_folder, nb_images)\n",
    "person_d_images = download_and_save_person_images(\"person_d\", tamara_taylor[0], images_folder, nb_images)\n",
    "\n",
    "\n",
    "create_testing_sets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faces(images):\n",
    "    faceCascade = cv2.CascadeClassifier(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"))\n",
    "    faces = []\n",
    "    for img in images:\n",
    "        img_ = img.copy()\n",
    "        img_gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
    "        faces_ = faceCascade.detectMultiScale(\n",
    "            img_gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        faces.append(faces_)\n",
    "        \n",
    "    print(\"Found {} face(s)!\".format(len(faces)))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_file_paths(base_path):\n",
    "    training_image_paths = []\n",
    "    testing_image_paths = []\n",
    "\n",
    "    training_folder = os.path.join(base_path, \"training\")\n",
    "    testing_folder = os.path.join(base_path, \"testing\")\n",
    "    \n",
    "    for image in os.listdir(training_folder):\n",
    "        training_image_paths.append(os.path.join(training_folder, image).replace(\"\\\\\", \"/\"))\n",
    "        \n",
    "    for image in os.listdir(testing_folder):\n",
    "        testing_image_paths.append(os.path.join(testing_folder, image).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "    return {\"training\": training_image_paths, \"testing\": testing_image_paths}\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_temp_training_testing_folder(file_paths):\n",
    "    if not os.path.isdir(tmp_train_folder):\n",
    "        os.mkdir(tmp_train_folder)\n",
    "    \n",
    "    if not os.path.isdir(tmp_test_folder):\n",
    "        os.mkdir(tmp_test_folder)\n",
    "\n",
    "    for image_path in file_paths[\"training\"]:\n",
    "        person_name = image_path.split(\"/\")[1]\n",
    "        destination_folder = os.path.join(tmp_train_folder, person_name)\n",
    "        \n",
    "        if not os.path.isdir(destination_folder):\n",
    "            os.mkdir(destination_folder)\n",
    "\n",
    "        if not os.path.isfile(os.path.join(tmp_test_folder, person_name, os.path.basename(image_path))):\n",
    "            copyfile(image_path, os.path.join(destination_folder, os.path.basename(image_path)))\n",
    "    \n",
    "    for image_path in file_paths[\"testing\"]:\n",
    "        person_name = image_path.split(\"/\")[1]\n",
    "        destination_folder = os.path.join(tmp_test_folder, person_name)\n",
    "        \n",
    "        if not os.path.isdir(destination_folder):\n",
    "            os.mkdir(destination_folder)\n",
    "\n",
    "        if not os.path.isfile(os.path.join(tmp_train_folder, person_name, os.path.basename(image_path))):\n",
    "            copyfile(image_path, os.path.join(destination_folder, os.path.basename(image_path)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "person_a_images_file_paths = get_image_file_paths(\"images/person_a/\")\n",
    "person_b_images_file_paths = get_image_file_paths(\"images/person_b/\")\n",
    "person_c_images_file_paths = get_image_file_paths(\"images/person_c/\")\n",
    "person_d_images_file_paths = get_image_file_paths(\"images/person_d/\")\n",
    "\n",
    "tmp_train_folder = os.path.join(base_path, \"tmp_train\");\n",
    "tmp_test_folder = os.path.join(base_path, \"tmp_test\");\n",
    "\n",
    "all_image_file_paths = {\n",
    "    'training': person_a_images_file_paths['training'] + person_b_images_file_paths['training'] + person_c_images_file_paths['training'] + person_d_images_file_paths['training'],\n",
    "    'testing': person_a_images_file_paths['testing'] + person_b_images_file_paths['testing' ] + person_c_images_file_paths['testing'] + person_d_images_file_paths['testing']\n",
    "}\n",
    "\n",
    "\n",
    "initialize_temp_training_testing_folder(all_image_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Load VGG19 base model\n",
    "    base_model = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(224, 224, 3),\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=\"softmax\",\n",
    "    )\n",
    "\n",
    "    # Freeze the layers of the base models\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create a new model with additional layers on top of the base_model\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(2048, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(2048, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(4, activation=\"softmax\")(x) \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 109 images belonging to 4 classes.\n",
      "Found 40 images belonging to 4 classes.\n",
      "WARNING:tensorflow:From s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:From s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3919 - accuracy: 0.2294\n",
      "Epoch 1: val_accuracy improved from -inf to 0.35000, saving model to ./models\\checkpoint_vgg16.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Development\\kdg\\Data\\5\\d-learning-transfer-learning\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 17s 17s/step - loss: 1.3919 - accuracy: 0.2294 - val_loss: 2.8767 - val_accuracy: 0.3500\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7048 - accuracy: 0.3119\n",
      "Epoch 2: val_accuracy did not improve from 0.35000\n",
      "1/1 [==============================] - 15s 15s/step - loss: 2.7048 - accuracy: 0.3119 - val_loss: 2.6168 - val_accuracy: 0.2500\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5667 - accuracy: 0.2661\n",
      "Epoch 3: val_accuracy did not improve from 0.35000\n",
      "1/1 [==============================] - 14s 14s/step - loss: 2.5667 - accuracy: 0.2661 - val_loss: 1.3666 - val_accuracy: 0.3000\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3398 - accuracy: 0.4220\n",
      "Epoch 4: val_accuracy did not improve from 0.35000\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.3398 - accuracy: 0.4220 - val_loss: 1.6187 - val_accuracy: 0.2500\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6433 - accuracy: 0.2294\n",
      "Epoch 5: val_accuracy improved from 0.35000 to 0.42500, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.6433 - accuracy: 0.2294 - val_loss: 1.5699 - val_accuracy: 0.4250\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5702 - accuracy: 0.3394\n",
      "Epoch 6: val_accuracy did not improve from 0.42500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.5702 - accuracy: 0.3394 - val_loss: 1.4697 - val_accuracy: 0.2500\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4377 - accuracy: 0.2477\n",
      "Epoch 7: val_accuracy did not improve from 0.42500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.4377 - accuracy: 0.2477 - val_loss: 1.3606 - val_accuracy: 0.4250\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2965 - accuracy: 0.4404\n",
      "Epoch 8: val_accuracy did not improve from 0.42500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.2965 - accuracy: 0.4404 - val_loss: 1.2860 - val_accuracy: 0.3500\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2159 - accuracy: 0.4404\n",
      "Epoch 9: val_accuracy did not improve from 0.42500\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.2159 - accuracy: 0.4404 - val_loss: 1.2516 - val_accuracy: 0.4000\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1946 - accuracy: 0.4404\n",
      "Epoch 10: val_accuracy did not improve from 0.42500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.1946 - accuracy: 0.4404 - val_loss: 1.2509 - val_accuracy: 0.3000\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2139 - accuracy: 0.2936\n",
      "Epoch 11: val_accuracy did not improve from 0.42500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.2139 - accuracy: 0.2936 - val_loss: 1.2229 - val_accuracy: 0.3000\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1841 - accuracy: 0.3119\n",
      "Epoch 12: val_accuracy improved from 0.42500 to 0.50000, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.1841 - accuracy: 0.3119 - val_loss: 1.1618 - val_accuracy: 0.5000\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1147 - accuracy: 0.4954\n",
      "Epoch 13: val_accuracy improved from 0.50000 to 0.52500, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.1147 - accuracy: 0.4954 - val_loss: 1.1140 - val_accuracy: 0.5250\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0526 - accuracy: 0.7339\n",
      "Epoch 14: val_accuracy did not improve from 0.52500\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.0526 - accuracy: 0.7339 - val_loss: 1.1025 - val_accuracy: 0.4250\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0062 - accuracy: 0.6972\n",
      "Epoch 15: val_accuracy did not improve from 0.52500\n",
      "1/1 [==============================] - 17s 17s/step - loss: 1.0062 - accuracy: 0.6972 - val_loss: 1.0972 - val_accuracy: 0.4500\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0006 - accuracy: 0.6422\n",
      "Epoch 16: val_accuracy did not improve from 0.52500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 1.0006 - accuracy: 0.6422 - val_loss: 1.0619 - val_accuracy: 0.4500\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9446 - accuracy: 0.7064\n",
      "Epoch 17: val_accuracy did not improve from 0.52500\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.9446 - accuracy: 0.7064 - val_loss: 0.9948 - val_accuracy: 0.5250\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8752 - accuracy: 0.7523\n",
      "Epoch 18: val_accuracy improved from 0.52500 to 0.62500, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.8752 - accuracy: 0.7523 - val_loss: 0.9239 - val_accuracy: 0.6250\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7973 - accuracy: 0.7431\n",
      "Epoch 19: val_accuracy improved from 0.62500 to 0.67500, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.7973 - accuracy: 0.7431 - val_loss: 0.8723 - val_accuracy: 0.6750\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7603 - accuracy: 0.7890\n",
      "Epoch 20: val_accuracy did not improve from 0.67500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.7603 - accuracy: 0.7890 - val_loss: 0.8360 - val_accuracy: 0.6250\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7304 - accuracy: 0.7615\n",
      "Epoch 21: val_accuracy did not improve from 0.67500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.7304 - accuracy: 0.7615 - val_loss: 0.8154 - val_accuracy: 0.6250\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.7798\n",
      "Epoch 22: val_accuracy did not improve from 0.67500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.6430 - accuracy: 0.7798 - val_loss: 0.8052 - val_accuracy: 0.6000\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6081 - accuracy: 0.7706\n",
      "Epoch 23: val_accuracy did not improve from 0.67500\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6081 - accuracy: 0.7706 - val_loss: 0.7847 - val_accuracy: 0.6500\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.7890\n",
      "Epoch 24: val_accuracy improved from 0.67500 to 0.72500, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6098 - accuracy: 0.7890 - val_loss: 0.7512 - val_accuracy: 0.7250\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.8257\n",
      "Epoch 25: val_accuracy did not improve from 0.72500\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5299 - accuracy: 0.8257 - val_loss: 0.7103 - val_accuracy: 0.7000\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4824 - accuracy: 0.8532\n",
      "Epoch 26: val_accuracy did not improve from 0.72500\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4824 - accuracy: 0.8532 - val_loss: 0.6882 - val_accuracy: 0.7250\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4707 - accuracy: 0.8165\n",
      "Epoch 27: val_accuracy improved from 0.72500 to 0.75000, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.4707 - accuracy: 0.8165 - val_loss: 0.6601 - val_accuracy: 0.7500\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4005 - accuracy: 0.8440\n",
      "Epoch 28: val_accuracy improved from 0.75000 to 0.80000, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.4005 - accuracy: 0.8440 - val_loss: 0.6636 - val_accuracy: 0.8000\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.9083\n",
      "Epoch 29: val_accuracy did not improve from 0.80000\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4012 - accuracy: 0.9083 - val_loss: 0.6919 - val_accuracy: 0.7500\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3835 - accuracy: 0.8532\n",
      "Epoch 30: val_accuracy did not improve from 0.80000\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3835 - accuracy: 0.8532 - val_loss: 0.6639 - val_accuracy: 0.7750\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3314 - accuracy: 0.8807\n",
      "Epoch 31: val_accuracy improved from 0.80000 to 0.82500, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.3314 - accuracy: 0.8807 - val_loss: 0.5912 - val_accuracy: 0.8250\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.9083\n",
      "Epoch 32: val_accuracy did not improve from 0.82500\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.2980 - accuracy: 0.9083 - val_loss: 0.5690 - val_accuracy: 0.8250\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9083\n",
      "Epoch 33: val_accuracy did not improve from 0.82500\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.2932 - accuracy: 0.9083 - val_loss: 0.5797 - val_accuracy: 0.8000\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.9174\n",
      "Epoch 34: val_accuracy did not improve from 0.82500\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.2617 - accuracy: 0.9174 - val_loss: 0.6168 - val_accuracy: 0.8000\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2688 - accuracy: 0.9450\n",
      "Epoch 35: val_accuracy did not improve from 0.82500\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.2688 - accuracy: 0.9450 - val_loss: 0.5989 - val_accuracy: 0.8250\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2229 - accuracy: 0.9358\n",
      "Epoch 36: val_accuracy did not improve from 0.82500\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.2229 - accuracy: 0.9358 - val_loss: 0.5644 - val_accuracy: 0.8250\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9541\n",
      "Epoch 37: val_accuracy did not improve from 0.82500\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.1995 - accuracy: 0.9541 - val_loss: 0.5502 - val_accuracy: 0.8250\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9541\n",
      "Epoch 38: val_accuracy did not improve from 0.82500\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.1775 - accuracy: 0.9541 - val_loss: 0.5367 - val_accuracy: 0.8250\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9633\n",
      "Epoch 39: val_accuracy improved from 0.82500 to 0.85000, saving model to ./models\\checkpoint_vgg16.h5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.1613 - accuracy: 0.9633 - val_loss: 0.5265 - val_accuracy: 0.8500\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1578 - accuracy: 0.9450\n",
      "Epoch 40: val_accuracy did not improve from 0.85000\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.1578 - accuracy: 0.9450 - val_loss: 0.5328 - val_accuracy: 0.8250\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              1050624   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 8196      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19969860 (76.18 MB)\n",
      "Trainable params: 5255172 (20.05 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Training data generator with augmentation\n",
    "trData = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Testing data generator (without augmentation)\n",
    "tsData = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "target_size = (224, 224)\n",
    "class_mode = 'categorical'\n",
    "epochs = 40\n",
    "learning_rate=0.001\n",
    "\n",
    "train_data = trData.flow_from_directory(\n",
    "    directory=tmp_train_folder,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode, \n",
    ")\n",
    "\n",
    "test_data = tsData.flow_from_directory(\n",
    "    directory=tmp_test_folder,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=class_mode\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./models/checkpoint_vgg16.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "callbacks = [checkpoint, early_stop]\n",
    "\n",
    "if os.path.isfile(\"./models/vgg16_custom.h5\"):\n",
    "    model = tf.keras.models.load_model(\"./models/vgg16_custom.h5\")\n",
    "else:\n",
    "    model = create_model()\n",
    "\n",
    "class_labels = list(test_data.class_indices.keys())\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "steps_per_epoch = len(train_data)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, epochs=epochs, validation_data=test_data, callbacks=callbacks, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"./models/vgg16_custom.h5\")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 4s 794ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate The Model\n",
    "\n",
    "\n",
    "test_batch = next(test_data)\n",
    "predictions = model.predict(test_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_predictions(class_labels,predictions):\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "        true_label =class_labels[np.argmax(test_batch[1][idx])]\n",
    "        plt.imshow(test_batch[0][idx])\n",
    "        plt.title(f\"Predicted: {predicted_label} - Actual: {true_label}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment If you want to display predictions\n",
    "# display_predictions(class_labels,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAG2CAYAAAAqWG/aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLY0lEQVR4nO3dfVyN9/8H8NdB5ZSKdCdyt0IRc0/mvo3NhO27fd0WhrmZQ6rRUOQm7FvYXcZMjDZsth82N2kYuQkJuUluJncJSY5Ida7fH+bMWbHO6TpdV6fXc4/r8XA+55zrep1zPc68fT6f63MpBEEQQERERGSASlIHICIiovKLhQQREREZjIUEERERGYyFBBERERmMhQQREREZjIUEERERGYyFBBERERmMhQQREREZjIUEERERGYyFBBERERmMhQQREZGJ+uOPP9C3b1+4uLhAoVDgl19+0XleEASEhoaiVq1aUCqV8PHxQVpaml7HYCFBRERkoh4+fIgWLVrgyy+/LPb5RYsW4bPPPsOyZctw+PBhWFlZoVevXnj8+HGJj6HgTbuIiIhMn0KhwM8//4z+/fsDeNob4eLigsDAQAQFBQEA7t+/DycnJ8TExGDgwIEl2i97JIiIiMqJvLw85OTk6Gx5eXkG7evy5cvIyMiAj4+Pts3W1hbt27fHwYMHS7yfKgYdXeZudesmdQR6Tu0D+o23kfG86dxS6gj0l20Zx6WOQH8peHLd6MfIv3NJlP1EfLEGs2fP1mkLCwvDrFmz9N5XRkYGAMDJyUmn3cnJSftcSZhkIUFERCQrmkJRdhMSEoIpU6botFlYWIiyb0OxkCAiIionLCwsRCscnJ2dAQC3bt1CrVq1tO23bt3Cq6++WuL9cI4EERGRsQkacTYRNWjQAM7OzoiPj9e25eTk4PDhw+jYsWOJ98MeCSIiImPTiFsElJRarcaFCxe0jy9fvozk5GTY2dmhbt26mDx5MubOnQt3d3c0aNAAM2fOhIuLi/bKjpJgIUFERGSijh49iu7du2sfP5tf4e/vj5iYGHz88cd4+PAhxowZg+zsbLz22mvYvn07qlatWuJjmOQ6ErxqQ1541YZ88KoN+eBVG/JRFldtPLlxWpT9mLs0FWU/YmKPBBERkbFJNLRRFjjZkoiIiAzGHgkiIiJjE/mKCzlhIUFERGRsIi1IJUcc2iAiIiKDsUeCiIjI2Di0QURERAYz4as2WEgQEREZmWDCPRKcI0FEREQGY48EERGRsXFog4iIiAzGoQ0iIiKiomTTI/Hjjz9iw4YNSE9Px5MnT3SeS0pKkigVERGRCLgglXF99tlnGDFiBJycnHD8+HG0a9cONWvWxKVLl/Dmm29KHY+IiKh0BI04mwzJopD46quvsHz5cnz++ecwNzfHxx9/jLi4OKhUKty/f1/qeERERPQCsigk0tPT4e3tDQBQKpV48OABAGDYsGH4/vvvpYxGRERUehqNOJsMyaKQcHZ2RlZWFgCgbt26OHToEADg8uXLEARBymhERESlx6EN4+rRowc2b94MABgxYgQCAgLw+uuv47///S8GDBggcToiIiJ6EVlctbF8+XJo/uqymTBhAmrWrIkDBw7A19cXH374ocTpiIiISkmmwxJikEUhUalSJVSq9HfnyMCBAzFw4MAirxs/fjzCw8Nhb29flvGIiIhKRRB4+acsrF27Fjk5OVLHICIi0g/nSMgDJ14SERHJiyyGNoiIiEwa50gQERGRwWQ6LCGGcjW0QURERPLCHgkiIiJjM+GbdpWrQmLo0KGwsbGROgYREZF+THhoQzaFRHZ2NhITE5GZmaldnOoZPz8/AEB0dLQU0YiIiOgFZFFIbNmyBUOGDIFarYaNjQ0UCoX2OYVCoS0kiIiIyiUTvmpDFpMtAwMDMXLkSKjVamRnZ+PevXva7dnNvIiIiMotLkhlXNevX4dKpYKlpaXUUYiIiEgPsigkevXqhaNHj0odg4iIyDg0GnE2GZLFHIk+ffogODgYZ86cgZeXF8zMzHSe9/X1lSgZERGRCGRaBIhBFoXE6NGjAQDh4eFFnlMoFCgsNN3rb4mIyPTx7p9GptFoXrhVxCKikr09bKZPh8P//R8cd+yA3bffokrjxlLHqrDGjfXHhfOHoM65iAP7t6Btm1eljlTh/GfCe4jaEoX1Zzbgu6S1mL5iOmo3rC11rAqNvwt6RhaFBP1NUa0a7L74AigowL2pU3HH3x/qr76C8OCB1NEqpPfe88X/Pg3DnLlRaNu+N06cPIPffl0HB4eaUkerUJq1b4ZfV/+K4P5BmDlkJipXqYLwtXNgobSQOlqFxN+FAUx4joRCkMm9uffu3Yv//e9/OHv2LADA09MTwcHB6Ny5s977utWtm8jpyk61MWNg1qwZ7qlUUkcRTe0DaVJHMNiB/Vtw5OgJTJo8A8DTobY/Lx3Bl1+twqJPv5Q4nf7edG4pdQRR2NjZYF1yLKb9ZypOJ56WOo5BtmUclzqCwUztd1Hw5LrRj/Fo9zei7EfZfZQo+xGTLHok1q5dCx8fH1haWkKlUkGlUkGpVKJnz56IjY2VOl6ZsvD2Rn5qKmxnzYLDzz/DbsUKKPv0kTpWhWRmZoZWrZoj/vd92jZBEBD/+3506NBawmRkZW0FAHiQrZY4ScXD3wX9kywmW86bNw+LFi1CQECAtk2lUiEqKgpz5szB4MGDX/jevLw85OXl6bZpNLCoJIsaSW+VXVxg2a8fcjdswL21a2HWpAmsVSoIBQV4vGOH1PEqFHt7O1SpUgWZt+7otGdm3kaTxq9IlIoUCgVGzxqNM0dOI/38FanjVDj8XRhIpsMSYpDF37aXLl1C3759i7T7+vri8uXLL31vREQEbG1tdbbP0tONFdX4FArknz8P9TffoODCBTzauhWPtm6FkpfAEgEAxs4dh7qN6mHRhEVSRyEqOa5saVyurq6Ij48v0r5r1y64urq+9L0hISG4f/++zqaqW9dYUY1Oc/cuCq/o/iur4MoVVHZ0lChRxXXnThYKCgrg6GSv0+7o6ICMW7clSlWxfRg+Fm17tsX0gZ/gbsZdqeNUSPxd0D/JYmgjMDAQKpUKycnJ8Pb2BgAkJCQgJiYGS5cufel7LSwsYGGhO3P7UTkd1gCAJykpqPyP4qmyqysKb92SKFHFlZ+fj6Skk+jR/TVs3vx0WEmhUKBH99fwVfQqidNVPB+Gj0XH3h0R8n4Ibl3l70Eq/F0YyISHNmRRSIwbNw7Ozs6IjIzEhg0bAAAeHh5Yv349+vXrJ3G6spW7cSPsvvwSlkOGIG/PHpg1aQLLt99GTmSk1NEqpMVLV2DVysU4lnQSR44ch2riaFhZKRGzer3U0SqUcXPHoUu/rpg3ai4ePcxFdYfqAIDcnFw8yXsibbgKiL8LA8h0WEIMsigkAGDAgAEYMGCA1DEkV5CaiuyZM1Ft9GhU8/dH4c2bePDFF3i8a5fU0SqkjRs3w8HeDrNCg+Ds7IATJ06jz9tDkZl559/fTKJ5y+/plUsRGxfotC+ZshjxPxYdFiXj4u+CnieLdSSuXr0KhUKBOnXqAAASExMRGxsLT09PjBkzRu/9led1JExReV5HwtSYyjoSpqA8ryNhaspkHYltn4myH+Wb8ltjSBaTCQYPHozdu3cDADIyMuDj44PExERMnz692PtvEBERlSsmvLKlLAqJlJQUtGvXDgCwYcMGeHl54cCBA1i3bh1iYmKkDUdERFRavPzTuPLz87VXXuzatUt72/AmTZrg5s2bUkYjIiKil5BFIdG0aVMsW7YM+/btQ1xcHHr37g0AuHHjBmrW5E1giIionOPQhnEtXLgQX3/9Nbp164ZBgwahRYsWAIDNmzdrhzyIiIjKLRMe2pD88k9BENCwYUOkp6ejoKAANWrU0D43ZswYWFpaSpiOiIiIXkYWhYSbmxtOnz4Nd3d3nefq168vTSgiIiIxyXRYQgySD21UqlQJ7u7uuHuX6+YTEZGJMuGhDckLCQBYsGABgoODkZKSInUUIiIi0oPkQxsA4Ofnh9zcXLRo0QLm5uZQKpU6z2dlZUmUjIiISAQmPLQhi0JiyZIlUkcgIiIyHhYSxuXv7y91BCIiIjKALOZIAMDFixcxY8YMDBo0CJmZmQCAbdu24fTp0xInIyIiKiVBEGeTIVkUEnv37oWXlxcOHz6MTZs2Qa1WAwBOnDiBsLAwidMRERGVEle2NK5p06Zh7ty5iIuLg7m5uba9R48eOHTokITJiIiIRMBCwrhOnTqFAQMGFGl3dHTEnTt3JEhEREREJSGLQqJ69erF3uXz+PHjqF27tgSJiIiIRMQFqYxr4MCBmDp1KjIyMqBQKKDRaJCQkICgoCD4+flJHY+IiKh0OLRhXPPnz0eTJk3g6uoKtVoNT09PdO7cGd7e3pgxY4bU8YiIiMqdwsJCzJw5Ew0aNIBSqcQrr7yCOXPmQBD56g9ZrCNhbm6OFStWIDQ0FKdOncLDhw/RsmVLuLm5SR2NiIio9CS4dHPhwoWIjo7G6tWr0bRpUxw9ehQjRoyAra0tVCqVaMeRRSEBACtXrsTixYuRlpYGAHB3d8fkyZMxatQoiZMRERGVkgTDEgcOHEC/fv3Qp08fAE/vqP39998jMTFR1OPIYmgjNDQUkyZNQt++fbFx40Zs3LgRffv2RUBAAEJDQ6WOR0REJAt5eXnIycnR2fLy8op9rbe3N+Lj43H+/HkAT9dm2r9/P958801RM8miRyI6OhorVqzAoEGDtG2+vr5o3rw5Jk6ciPDwcAnTERERlZJIPRIRERGYPXu2TltYWBhmzZpV5LXTpk1DTk4OmjRpgsqVK6OwsBDz5s3DkCFDRMnyjCwKifz8fLRp06ZIe+vWrVFQUCBBIiIiIhGJdOlmSEgIpkyZotNmYWFR7Gs3bNiAdevWITY2Fk2bNkVycjImT54MFxcXUe9xJYtCYtiwYYiOjkZUVJRO+/Lly0WvnIiIiMorCwuLFxYO/xQcHIxp06Zh4MCBAAAvLy9cuXIFERERpldIAE8nW+7cuRMdOnQAABw+fBjp6enw8/PTqb7+WWwQERHJnaAp+6s2cnNzUamS7lTIypUrQyPyxE9ZFBIpKSlo1aoVgKd3AQUAe3t72NvbIyUlRfs6hUIhST4iIqJSkeCqjb59+2LevHmoW7cumjZtiuPHjyMqKgojR44U9TiyKCR2794tdQQiIiLjkWB5688//xwzZ87E+PHjkZmZCRcXF3z44YeiXw0pi0KCiIiIxGVtbY0lS5ZgyZIlRj0OCwkiIiJjk2CORFlhIUFERGRsMr3hlhhksbIlERERlU/skSAiIjI2E+6RYCFBRERkbBLc/bOscGiDiIiIDMYeCSIiImPj0AYREREZjJd/EhERkcEkWNmyrHCOBBERERmMPRJERETGxqGN8qX2gTSpI9BzHt3YJ3UE+ovSpbPUEYgqJMGEJ1tyaIOIiIgMZpI9EkRERLLCoQ0iIiIyGK/aICIiIiqKPRJERETGxqENIiIiMhiv2iAiIiIqij0SRERExsahDSIiIjKYCV+1wUKCiIjI2Ey4R4JzJIiIiMhg7JEgIiIyMlO+1wYLCSIiImPj0AYRERFRUeyRICIiMjYT7pFgIUFERGRsJnz5J4c2iIiIyGDskSAiIjI2Dm0YX2FhIX7++WecPXsWAODh4YH+/fujShXZRCQiIjKIwELCuE6fPg1fX19kZGSgcePGAICFCxfCwcEBW7ZsQbNmzSROSERERMWRxRyJUaNGoWnTprh27RqSkpKQlJSEq1evonnz5hgzZozU8YiIiEpHI4izyZAseiSSk5Nx9OhR1KhRQ9tWo0YNzJs3D23btpUwGRERkQhMeGVLWfRINGrUCLdu3SrSnpmZCTc3NwkSERERiciEeyQkKyRycnK0W0REBFQqFX788Udcu3YN165dw48//ojJkydj4cKFUkUkIiKifyHZ0Eb16tWhUCi0jwVBwPvvv69tE4SnlVffvn1RWFgoSUYiIiJRyLQ3QQySFRK7d++W6tBERERl6tk/jk2RZIVE165d9X7P+PHjER4eDnt7eyMkIiIiIn3JYrJlSa1duxY5OTlSxyAiItKPCU+2lMXlnyVlyl1DRERkwmRaBIihXPVIEBERkbyUqx4JIiKi8oj32iAiIiLDmXAhwaENIiIiMli56pEYOnQobGxspI5BRESkH9O91YZ8Cons7GwkJiYiMzMTmn/c3MTPzw8AEB0dLUU0IiKiUuEcCSPbsmULhgwZArVaDRsbG52lsxUKhbaQICIiKpdMuJCQxRyJwMBAjBw5Emq1GtnZ2bh37552y8rKkjoeERERvYAseiSuX78OlUoFS0tLqaMQERGJz4TnSMiiR6JXr144evSo1DGIiIiMQtAIomxyJIseiT59+iA4OBhnzpyBl5cXzMzMdJ739fWVKBkRERG9jCx6JEaPHo2rV68iPDwc7733Hvr376/dBgwYIHU8SYwb648L5w9BnXMRB/ZvQds2r0odyeQdTT6FCR+HobvvEDTr9Cbi/zig87wgCPhixRp08x2M1t37YdSkEFy5el2itBUTfxfywXOhJ41ImwzJopDQaDQv3AoLC6WOV+bee88X//s0DHPmRqFt+944cfIMfvt1HRwcakodzaQ9evQYjd0aYnrg+GKf/3bdRqz7cTNCgycidsUSKKtWxYdTZiAv70kZJ62Y+LuQD54L/Zny0IYsCgnSFTBpNL5ZGYvVazbg7Nk0jJ8wDbm5jzBi+ECpo5m0zh3bQjXGHz5dOxV5ThAEfLfhF4zxH4genTuisVsDzJ8ZhMw7dxG/70AxeyOx8XchHzwX9DzZFBJ79+5F37594ebmBjc3N/j6+mLfvn1SxypzZmZmaNWqOeJ///uzC4KA+N/3o0OH1hImq9iu3cjAnbv30LFNS22bdTUrNPdsjBMp5yRMVjHwdyEfPBcG4tCGca1duxY+Pj6wtLSESqWCSqWCUqlEz549ERsbK3W8MmVvb4cqVaog89YdnfbMzNtwdnKQKBXdyboHAKhpV0OnvaZdDdy5e0+KSBUKfxfywXNhGEEjziZHsrhqY968eVi0aBECAgK0bSqVClFRUZgzZw4GDx78wvfm5eUhLy9Pp00QBJ3VMYmIiMg4ZNEjcenSJfTt27dIu6+vLy5fvvzS90ZERMDW1lZnEzQPjBXV6O7cyUJBQQEcnex12h0dHZBx67ZEqcj+r56Iu1m6vQ93s+7BvmaN4t5CIuLvQj54LgzEoQ3jcnV1RXx8fJH2Xbt2wdXV9aXvDQkJwf3793U2RSVrY0U1uvz8fCQlnUSP7q9p2xQKBXp0fw2HDh2TMFnFVsfFGfY1a+DQsWRtm/rhQ5w8k4oWzZpIF6yC4O9CPnguDMOhDSMLDAyESqVCcnIyvL29AQAJCQmIiYnB0qVLX/peCwsLWFhY6LSV92GNxUtXYNXKxTiWdBJHjhyHauJoWFkpEbN6vdTRTFpu7iOkX7uhfXz9xi2cO38RtjbWqOXsiGHv98fy1T+gXp3aqO3ihC9WfAdH+5ro2dlbwtQVB38X8sFzYQCZFgFikEUhMW7cODg7OyMyMhIbNmwAAHh4eGD9+vXo16+fxOnK3saNm+Fgb4dZoUFwdnbAiROn0eftocjMvPPvbyaDpZxLw8iJU7WPF32+HADQ700fzJsRiJFD3sOjR48xa9FneKBWo1XzplgWOQcWFuZSRa5Q+LuQD54Lep5CEAR5rnBRClXMa0sdgZ7z6EbFu4xXrpQunaWOQCQ7BU+Mv0Lt7de7irIfh7i9ouxHTLKYI3H16lVcu3ZN+zgxMRGTJ0/G8uXLJUxFREQkDqnmSFy/fh1Dhw5FzZo1oVQq4eXlJfpNMmVRSAwePBi7d+8GAGRkZMDHxweJiYmYPn06wsPDJU5HRERU/ty7dw+dOnWCmZkZtm3bhjNnziAyMhI1aoh7pZlBcyQ0Gg0uXLiAzMxMaDS6JVKXLl303l9KSgratWsHANiwYQO8vLyQkJCAnTt3YuzYsQgNDTUkJhERkSxIccXFwoUL4erqilWrVmnbGjRoIPpx9C4kDh06hMGDB+PKlSv45/QKhUJh0E228vPztVde7Nq1S3vb8CZNmuDmzZt674+IiEhWBHGuJixuEcbirl4EgM2bN6NXr1547733sHfvXtSuXRvjx4/H6NGjRcnyjN5DG2PHjkWbNm2QkpKCrKws3Lt3T7tlZWUZFKJp06ZYtmwZ9u3bh7i4OPTu3RsAcOPGDdSsybvJERERAcUvwhgREVHsay9duoTo6Gi4u7tjx44dGDduHFQqFVavXi1qJr2v2rCyssKJEyfg5uYmWog9e/ZgwIAByMnJgb+/P7799lsAwCeffIJz585h06ZNeu2PV23IC6/akA9etUFUVFlctZHRpZso+6kRt6PEPRLm5uZo06YNDhz4+w7FKpUKR44cwcGDB0XJAxgwtNG+fXtcuHBBtEJCEAQ0bNgQ6enpKCgo0JkEMmbMGFhaWopyHCIiIqkIGnGGNl5UNBSnVq1a8PT01Gnz8PDATz/9JEqWZ/QuJCZOnIjAwEBkZGTAy8sLZmZmOs83b95cr/0JggA3NzecPn0a7u7uOs/Vr19f33hEREQEoFOnTkhNTdVpO3/+POrVqyfqcfQuJN59910AwMiRI7VtCoVCe8dNfSdbVqpUCe7u7rh7926RQoKIiMgUSHHVRkBAALy9vTF//ny8//77SExMxPLly0Vfo0nvQuLf7sZpiAULFiA4OBjR0dFo1qyZ6PsnIiKSkiDSVRv6aNu2LX7++WeEhIQgPDwcDRo0wJIlSzBkyBBRjyOLJbJr1KiB3NxcFBQUwNzcHEqlUud5fa8G4WRLeeFkS/ngZEuiospisuW19j1E2U+dw7+Lsh8xGbQg1cWLF7FkyRKcPXsWAODp6YlJkybhlVdeMSjEkiVLDHofERERSUvvQmLHjh3w9fXFq6++ik6dOgF4esvvpk2bYsuWLXj99df1DuHv76/3e4iIiMoLsa7akCO9C4lp06YhICAACxYsKNI+depUgwoJ4Gkvx6pVq3Dx4kUsXboUjo6O2LZtG+rWrYumTZsatE8iIiI5kH4SgfHovbLl2bNn8cEHHxRpHzlyJM6cOWNQiL1798LLywuHDx/Gpk2boFarAQAnTpxAWFiYQfskIiIi49O7kHBwcEBycnKR9uTkZDg6OhoUYtq0aZg7dy7i4uJgbm6ube/RowcOHTpk0D6JiIjkQtAoRNnkSO+hjdGjR2PMmDG4dOkSvL29ATydI7Fw4UJMmTLFoBCnTp1CbGxskXZHR0fcuXPHoH0SERHJhVyLADHoXUjMnDkT1tbWiIyMREhICADAxcUFs2bNgkqlMihE9erVcfPmzSK3Nz1+/Dhq1+alnERERHKldyGhUCgQEBCAgIAAPHjwAABgbW1dqhADBw7E1KlTsXHjRigUCmg0GiQkJCAoKAh+fn6l2jcREZHUONnyOY8ePUJubi6ApwVEVlYWlixZgp07dxocYv78+WjSpAlcXV2hVqvh6emJzp07w9vbGzNmzDB4v0RERHJgynMk9F7Z8o033sA777yDsWPHIjs7G40bN4a5uTnu3LmDqKgojBs3zuAwV69exalTp/Dw4UO0bNnS4DuMcmVLeeHKlvLBlS2JiiqLlS0veb0hyn4anjL8H+3GonePRFJSEjp3fvo/ox9//BHOzs64cuUK1qxZg88++8zgICtXrsSbb76JAQMGYOjQoejfvz+++eYbg/dHREQkF4KgEGWTI73nSOTm5mrnROzcuRPvvPMOKlWqhA4dOuDKlSsGhQgNDUVUVBQmTpyIjh07AgAOHjyIgIAApKenIzw83KD9EhERyYEUd/8sK3oXEm5ubvjll18wYMAA7NixAwEBAQCAzMxM2NjYGBQiOjoaK1aswKBBg7Rtvr6+aN68OSZOnMhCgoiIyjWNTHsTxKD30EZoaCiCgoJQv359tG/fXtuDsHPnTrRs2dKgEPn5+WjTpk2R9tatW6OgoMCgfRIREZHx6V1I/Oc//0F6ejqOHj2K7du3a9t79uyJxYsXGxRi2LBhiI6OLtK+fPly0e+bTkREVNY4R+Iv+fn5UCqVSE5OLtL70K5du1IFWblyJXbu3IkOHToAAA4fPoz09HT4+fnprJgZFRVVquMQERGVNbleuikGvQoJMzMz1K1bF4WFhaKGSElJQatWrQA8vQsoANjb28Pe3h4pKSna1ykUpnsiiIiIyiO9J1tOnz4dn3zyCb777jvY2dmJEmL37t2i7IeIiEiOTHllS70LiS+++AIXLlyAi4sL6tWrBysrK53nk5KSRAtHRERkCji08Zz+/fsbIQYRERGVR3oXEmFhYcbIQUREZLK4jsQ/ZGdn45tvvkFISAiysrIAPB3SuH7d+OuVExERlTe8/PM5J0+ehI+PD2xtbfHnn39i9OjRsLOzw6ZNm5Ceno41a9YYIycRERHJkN49ElOmTMHw4cORlpaGqlWratvfeust/PHHH6KGIyIiMgWCIM4mR3r3SBw5cgRff/11kfbatWsjIyNDlFBERESmxJTnSOhdSFhYWCAnJ6dI+/nz5+Hg4CBKKCIiIlMi1/kNYtB7aMPX1xfh4eHIz88H8HS1yfT0dEydOhXvvvuu6AGJiIhIvvQuJCIjI6FWq+Ho6IhHjx6ha9eucHNzg7W1NebNm2eMjEREROUa50g8x9bWFnFxcUhISMCJEyegVqvRqlUr+Pj4GCMfERFRucc5En9Zv349Nm/ejCdPnqBnz54YP368sXIRERFROVDiQiI6OhoTJkyAu7s7lEolNm3ahIsXL+LTTz81Zj6DfOvQXeoI9BylS2epI9Bfsj9qLXUE+kv1L45JHYHKECdb4unNusLCwpCamork5GSsXr0aX331lTGzERERmQSNoBBlk6MSFxKXLl2Cv7+/9vHgwYNRUFCAmzdvGiUYERERyV+Jhzby8vJ0bhleqVIlmJub49GjR0YJRkREZCpkesGFKPSabDlz5kxYWlpqHz958gTz5s2Dra2tti0qKkq8dERERCZArsMSYihxIdGlSxekpqbqtHl7e+PSpUvaxwqF6X5RREREVFSJC4k9e/YYMQYREZHpMuWrNvRekIqIiIj0o5E6gBGxkCAiIjIyAabbI6H3vTaIiIiInmGPBBERkZFpTPj6TxYSRERERqbh0Iauffv2YejQoejYsSOuX78OAPjuu++wf/9+UcMRERGRvOldSPz000/o1asXlEoljh8/jry8PADA/fv3MX/+fNEDEhERlXcCFKJscqR3ITF37lwsW7YMK1asgJmZmba9U6dOSEpKEjUcERGRKdCItMmR3oVEamoqunTpUqTd1tYW2dnZYmQiIiKickLvQsLZ2RkXLlwo0r5//340bNhQlFBERESmhEMbzxk9ejQmTZqEw4cPQ6FQ4MaNG1i3bh2CgoIwbtw4Y2QkIiIq10x5aEPvyz+nTZsGjUaDnj17Ijc3F126dIGFhQWCgoIwceJEY2QkIiIimdK7kFAoFJg+fTqCg4Nx4cIFqNVqeHp6olq1agaHuHz5MgoKCuDu7q7TnpaWBjMzM9SvX9/gfRMREUlNrr0JYjB4iWxzc3N4enqiXbt2pSoiAGD48OE4cOBAkfbDhw9j+PDhpdo3ERGR1Ex5joTePRLdu3eHQvHiD/P777/rHeL48ePo1KlTkfYOHTrgo48+0nt/REREcqKRZw0gCr0LiVdffVXncX5+PpKTk5GSkgJ/f3+DQigUCjx48KBI+/3791FYWGjQPomIiMj49C4kFi9eXGz7rFmzoFarDQrRpUsXRERE4Pvvv0flypUBAIWFhYiIiMBrr71m0D6JiIjkwpTvtSHaTbuGDh2Kdu3a4X//+5/e7124cCG6dOmCxo0bo3PnzgCe3s8jJyfHoKESIiIiOTHhm38aPtnynw4ePIiqVasa9F5PT0+cPHkS77//PjIzM/HgwQP4+fnh3LlzaNasmVgRiYiISGR690i88847Oo8FQcDNmzdx9OhRzJw50+AgLi4u/3rTr/HjxyM8PBz29vYGH4eIiKis8fLP59ja2upsdnZ26NatG3777TeEhYUZI6PW2rVrkZOTY9RjEBERiU2jUIiyyZFePRKFhYUYMWIEvLy8UKNGDWNleiFBMOVRJiIiovJHrx6JypUr44033uBdPomIiPQgiLTJkd5DG82aNcOlS5eMkYWIiMgkmfJNu/QuJObOnYugoCBs3boVN2/eRE5Ojs5GREREFUeJ50iEh4cjMDAQb731FgDA19dXZ6lsQRCgUCi4EiUREdE/cIlsALNnz8bYsWOxe/duY+Z5qaFDh8LGxkay4xMRERlCDitbLliwACEhIZg0aRKWLFki2n5LXEg8u2Kia9euoh38ednZ2UhMTERmZiY0Gt2RID8/PwBAdHS0UY5NRERkTFJPlDxy5Ai+/vprNG/eXPR963X558vu+lkaW7ZswZAhQ6BWq2FjY6NzHIVCoS0kiIiISD9qtRpDhgzBihUrMHfuXNH3r1ch0ahRo38tJrKysvQOERgYiJEjR2L+/PmwtLTU+/1ERERyJtYciby8POTl5em0WVhYwMLC4oXvmTBhAvr06QMfHx/pC4nZs2fD1tZW9BDXr1+HSqViEUFERCZJrEs3IyIiMHv2bJ22sLAwzJo1q9jX//DDD0hKSsKRI0dESlCUXoXEwIED4ejoKHqIXr164ejRo2jYsKHo+yYiIjIVISEhmDJlik7bi3ojrl69ikmTJiEuLs7gm2qWRIkLCWPNjwCAPn36IDg4GGfOnIGXlxfMzMx0nvf19TXasYmIiIxNrMmW/zaM8bxjx44hMzMTrVq10rYVFhbijz/+wBdffIG8vDxUrly51Jn0vmrDGEaPHg3g6VoV/8S1KYiIqLyTYh2Jnj174tSpUzptI0aMQJMmTTB16lRRighAj0Lin5dkismY+y7Pmk3oi1af/BdnvtmOo2FrpY5TYY0b64/AKePg7OyAkyfPYNLkmThyNFnqWBWK5YwVqGTnVKT9yf5f8WTT1xIkIv4u5M/a2hrNmjXTabOyskLNmjWLtJeGXnMkqOzUbNEQ7kO7I+vMFamjVGjvveeL/30ahvETpiHxyHGoJo7Cb7+ug2ezLrh9+67U8SqM3MWBUFT6e0X/Ss71oBw3B4UnEiRMVXHxd6E/U/7nst732jCWvXv3om/fvnBzc4Obmxt8fX2xb98+qWNJooqlBTp/MQ6HPl6JJ9m5Usep0AImjcY3K2Oxes0GnD2bhvETpiE39xFGDB8odbSK5WEOhAfZ2q1y07bQ3LmJwospUierkPi70J9cbtq1Z88eUVe1BGRSSKxduxY+Pj6wtLSESqWCSqWCUqlEz549ERsbK3W8Mtd+/nBci0/GzX2npY5SoZmZmaFVq+aI//3vglYQBMT/vh8dOrSWMFkFV7kKzFp1Q/7hXVInqZD4u6B/ksXQxrx587Bo0SIEBARo21QqFaKiojBnzhwMHjz4he8tbnGOfKEQZgpxJpGUtfq+HWDXrD5+7RMqdZQKz97eDlWqVEHmrTs67ZmZt9Gk8SsSpaIqzdoDSisUHImXOkqFxN+FYQTpb7VhNLLokbh06RL69u1bpN3X1xeXL19+6XsjIiJga2urs219UD7/JW/pYoe24cOwb+JX0OTlSx2HSJaqtH8dheeOQcjRfxVdIqnIZWjDGGTRI+Hq6or4+Hi4ubnptO/atQuurq4vfW9xi3NsbPKh6BnLQk2vBlA62OLt7X8vYVqpSmU4dWiMJsNfx7oGwyFopL71S8Vx504WCgoK4Ohkr9Pu6OiAjFu3JUpVsSlqOKByoxZ4vGqB1FEqLP4uDCPXIkAMsigkAgMDoVKpkJycDG9vbwBAQkICYmJisHTp0pe+t7jFOcrrsMbN/aexucc0nTbvqDG4f/EGTn+5lUVEGcvPz0dS0kn06P4aNm/eAeDpuiY9ur+Gr6JXSZyuYjJr5wNBfR+FZ4233C+9HH8X9E+yKCTGjRsHZ2dnREZGYsOGDQAADw8PrF+/Hv369ZM4XdkpePgY2anXdNty85B3T12kncrG4qUrsGrlYhxLOokjR45DNXE0rKyUiFm9XupoFY9CgSpte6LgyO8A156RFH8X+jPlfwbKopAAgAEDBmDAgAFSxyDSsXHjZjjY22FWaBCcnR1w4sRp9Hl7KDIz7/z7m0lUld1boJKdI/ITebWG1Pi70J8UK1uWFYVgzLWvS+jq1atQKBSoU6cOACAxMRGxsbHw9PTEmDFj9N7fmtpDxY5IpTDy9m6pI9Bfsj/i5XlyUf2LY1JHoL8UPLlu9GMsrSvO30uT0uW3yrEsrtoYPHgwdu9++pdNRkYGfHx8kJiYiOnTpxd7/w0iIqLyxJSv2pBFIZGSkoJ27doBADZs2AAvLy8cOHAA69atQ0xMjLThiIiISomFhJHl5+drr7zYtWuX9rbhTZo0wc2bN6WMRkRERC8hi0KiadOmWLZsGfbt24e4uDj07t0bAHDjxg3UrFlT4nRERESlI4i0yZEsComFCxfi66+/Rrdu3TBo0CC0aNECALB582btkAcREVF5pVGIs8mR5Jd/CoKAhg0bIj09HQUFBahRo4b2uTFjxsDS0lLCdERERPQykvdICIIANzc3ZGRk6BQRAFC/fn04OjpKlIyIiEgcnGxpzACVKsHd3R13796VOgoREZFRcI6EkS1YsADBwcFISUmROgoREZHoNBBE2eRI8jkSAODn54fc3Fy0aNEC5ubmUCqVOs9nZfF2wURERHIki0JiyZIlUkcgIiIyGrnObxCDLAoJf39/qSMQEREZjTwHJcQhizkSAHDx4kXMmDEDgwYNQmZmJgBg27ZtOH36tMTJiIiI6EVkUUjs3bsXXl5eOHz4MDZt2gS1Wg0AOHHiBMLCwiROR0REVDq8/NPIpk2bhrlz5yIuLg7m5uba9h49euDQoUMSJiMiIio9U17ZUhaFxKlTpzBgwIAi7Y6Ojrhz544EiYiIiKgkZFFIVK9evdi7fB4/fhy1a9eWIBEREZF4THkdCVkUEgMHDsTUqVORkZEBhUIBjUaDhIQEBAUFwc/PT+p4REREpcKVLY1s/vz5aNKkCVxdXaFWq+Hp6YnOnTvD29sbM2bMkDoeERERvYAs1pEwNzfHihUrEBoailOnTuHhw4do2bIl3NzcpI5GRERUanK94kIMsigkAGDlypVYvHgx0tLSAADu7u6YPHkyRo0aJXEyIiKi0pHr/AYxyKKQCA0NRVRUFCZOnIiOHTsCAA4ePIiAgACkp6cjPDxc4oRERESGM90yQiaFRHR0NFasWIFBgwZp23x9fdG8eXNMnDiRhQQREZFMyaKQyM/PR5s2bYq0t27dGgUFBRIkIiIiEo8pz5GQxVUbw4YNQ3R0dJH25cuXY8iQIRIkIiIiEo8pryMhix4J4Olky507d6JDhw4AgMOHDyM9PR1+fn6YMmWK9nVRUVFSRSQiIqJ/kEUhkZKSglatWgF4ehdQALC3t4e9vT1SUlK0r1MoZLrQOBER0UvIsy9BHLIoJHbv3i11BCIiIqPhHAkiIiKiYsiiR4KIiMiUCSY8uMFCgoiIyMg4tEFERERUDPZIEBERGZlc14AQAwsJIiIiIzPdMoKFBBERkdGZco8E50gQERGRwdgjQUREZGSmfNUGCwkiIiIjM+V1JDi0QURERAZjjwQREZGRcWijnBl5mzcBIypOq+9uSB2B/vLoxj6pI1AZ4tAGERERUTFMskeCiIhITji0QURERAbTCBzaICIiIiqCPRJERERGZrr9ESwkiIiIjM6U77XBQoKIiMjIePknERERUTHYI0FERGRkvPyTiIiIDGbKcyQ4tEFEREQGY48EERGRkZnyZEsWEkREREZmynMkOLRBREREBmMhQUREZGSCIIiy6SMiIgJt27aFtbU1HB0d0b9/f6Smpor+2VhIEBERGZkGgiibPvbu3YsJEybg0KFDiIuLQ35+Pt544w08fPhQ1M/GORJEREQmaPv27TqPY2Ji4OjoiGPHjqFLly6iHYeFBBERkZGJNdkyLy8PeXl5Om0WFhawsLD41/fev38fAGBnZydSmqckG9rIyckp8UZERFSeCSL9FxERAVtbW50tIiLiX4+v0WgwefJkdOrUCc2aNRP1s0nWI1G9enUoFIoSvbawsNDIaYiIiIxHrJUtQ0JCMGXKFJ22kvRGTJgwASkpKdi/f78oOZ4nWSGxe/du7Z///PNPTJs2DcOHD0fHjh0BAAcPHsTq1atLVGkRERFVBCUdxnjeRx99hK1bt+KPP/5AnTp1RM8kWSHRtWtX7Z/Dw8MRFRWFQYMGadt8fX3h5eWF5cuXw9/fX4qIREREotD30k2xjjlx4kT8/PPP2LNnDxo0aGCU48ji8s+DBw+iTZs2RdrbtGmDxMRECRIRERGJRyPSpo8JEyZg7dq1iI2NhbW1NTIyMpCRkYFHjx6J8ZG0ZFFIuLq6YsWKFUXav/nmG7i6ukqQiIiIqHyLjo7G/fv30a1bN9SqVUu7rV+/XtTjyOLyz8WLF+Pdd9/Ftm3b0L59ewBAYmIi0tLS8NNPP0mcjoiIqHSkuGlXWQ2nyKJH4q233kJaWhp8fX2RlZWFrKws9O3bF+fPn8dbb70ldTwiIqJSkWJly7Iiix4JAKhTpw7mzZv30teMHz8e4eHhsLe3L6NURERE9DKy6JEoqbVr13KBKiIiKnekuGlXWZFNj0RJyPVLJCIiehm5DkuIoVz1SBAREZG8lKseCSIiovJIiqs2ygoLCSIiIiPTmPDQPAsJIiIiIzPdMqKczZEYOnQobGxspI5BREREf5FNj0R2djYSExORmZkJjUZ3RXE/Pz8AT5f7JCIiKm9M+aoNWRQSW7ZswZAhQ6BWq2FjYwOFQqF9TqFQaAsJIiKi8siUCwlZDG0EBgZi5MiRUKvVyM7Oxr1797RbVlaW1PGIiIjoBWTRI3H9+nWoVCpYWlpKHYWIiEh0prygoix6JHr16oWjR49KHYOIiMgoeNMuI+vTpw+Cg4Nx5swZeHl5wczMTOd5X19fiZIRERHRyygEGfS3VKr04o4RhUKBwsJCvfZXxbx2aSNJbtxYfwROGQdnZwecPHkGkybPxJGjyVLHqpBM6Vw0tK0ldQSDtenYEqMmDEPTFh5wcnbAeL9A7Nq2V+pYBjt9doPUEUrkaPIprIr9EWfOXcDtu1lYGjETPbt4a58XBAFffvMdftyyHQ8ePETL5p6YGfQR6rmWn/8Pm9k3NPox2rp0EWU/R278Icp+xCSLoQ2NRvPCTd8iwhS8954v/vdpGObMjULb9r1x4uQZ/PbrOjg41JQ6WoXDcyEflpZKnDudhvCpC6WOUqE8evQYjd0aYnrg+GKf/3bdRqz7cTNCgycidsUSKKtWxYdTZiAv70kZJ5U3U777pywKCdIVMGk0vlkZi9VrNuDs2TSMnzANubmPMGL4QKmjVTg8F/LxR/wBLImIRtxve6SOUqF07tgWqjH+8OnaqchzgiDguw2/YIz/QPTo3BGN3Rpg/swgZN65i/h9ByRIK1+mPEdCNoXE3r170bdvX7i5ucHNzQ2+vr7Yt2+f1LHKnJmZGVq1ao743//+7IIgIP73/ejQobWEySoenguil7t2IwN37t5DxzYttW3W1azQ3LMxTqSckzAZlSVZFBJr166Fj48PLC0toVKpoFKpoFQq0bNnT8TGxr70vXl5ecjJydHZ5Nr9UxL29naoUqUKMm/d0WnPzLwNZycHiVJVTDwXRC93J+seAKCmXQ2d9pp2NXDn7j0pIsmWKQ9tyOKqjXnz5mHRokUICAjQtqlUKkRFRWHOnDkYPHjwC98bERGB2bNn67QpKlWDojLvyUFERPIg12EJMciiR+LSpUvo27dvkXZfX19cvnz5pe8NCQnB/fv3dTZFJWtjRTW6O3eyUFBQAEcne512R0cHZNy6LVGqionngujl7P/qibibpdv7cDfrHuxr1ijuLWSCZFFIuLq6Ij4+vkj7rl274Orq+tL3WlhYwMbGRmd7/l4d5U1+fj6Skk6iR/fXtG0KhQI9ur+GQ4eOSZis4uG5IHq5Oi7OsK9ZA4eOJWvb1A8f4uSZVLRo1kS6YDIkiPSfHMliaCMwMBAqlQrJycnw9n56fXJCQgJiYmKwdOlSidOVvcVLV2DVysU4lnQSR44ch2riaFhZKRGzer3U0Socngv5sLRSol6Dv/9hUadubXg0a4Tse/dx8/otCZOZttzcR0i/dkP7+PqNWzh3/iJsbaxRy9kRw97vj+Wrf0C9OrVR28UJX6z4Do72NdGzs/dL9lrxaGQ6v0EMsigkxo0bB2dnZ0RGRmLDhqeLtHh4eGD9+vXo16+fxOnK3saNm+Fgb4dZoUFwdnbAiROn0eftocjMvPPvbyZR8VzIR7MWnlj7f19rH38ydwoAYNMPWzBt4uwXvY1KKeVcGkZOnKp9vOjz5QCAfm/6YN6MQIwc8h4ePXqMWYs+wwO1Gq2aN8WyyDmwsDCXKjKVMVmsbCk2U1jZksgYyvPKlqamvKxsWRGUxcqWTZ3ai7Kf07cOi7IfMcmiR+Lq1atQKBSoU6cOACAxMRGxsbHw9PTEmDFjJE5HRERUOqY8tCGLyZaDBw/G7t27AQAZGRnw8fFBYmIipk+fjvDwcInTERER0YvIopBISUlBu3btAAAbNmyAl5cXDhw4gHXr1iEmJkbacERERKXEqzaMLD8/HxYWFgCeXvL57LbhTZo0wc2bN6WMRkREVGoc2jCypk2bYtmyZdi3bx/i4uLQu3dvAMCNGzdQsybvskhEROWbKfdIyKKQWLhwIb7++mt069YNgwYNQosWLQAAmzdv1g55EBERkfxIPrQhCAIaNmyI9PR0FBQUoEaNv5dVHTNmDCwtLSVMR0REVHoc2jAiQRDg5uaGjIwMnSICAOrXrw9HR0eJkhEREYmDQxvGDFCpEtzd3XH37l2poxAREZGeJC8kAGDBggUIDg5GSkqK1FGIiIhEJwgaUTY5knyOBAD4+fkhNzcXLVq0gLm5OZRKpc7zWVlZEiUjIiIqPY1MhyXEIItCYsmSJVJHICIiIgPIopDw9/eXOgIREZHRmOD9MbVkMUcCAC5evIgZM2Zg0KBByMzMBABs27YNp0+fljgZERFR6WggiLLJkSwKib1798LLywuHDx/Gpk2boFarAQAnTpxAWFiYxOmIiIjoRWRRSEybNg1z585FXFwczM3Nte09evTAoUOHJExGRERUeoIgiLLJkSzmSJw6dQqxsbFF2h0dHXHnzh0JEhEREYmHK1saWfXq1Yu9y+fx48dRu3ZtCRIRERGJhytbGtnAgQMxdepUZGRkQKFQQKPRICEhAUFBQfDz85M6HhEREb2ALAqJ+fPno0mTJnB1dYVarYanpyc6d+4Mb29vzJgxQ+p4REREpWLKcyQUgoySXb16FadOncLDhw/RsmVLuLm5GbSfKuYcDiEqTkPbWlJHoL+cPrtB6gj0FzP7hkY/hoNtY1H2c/t+qij7EZMsJlsCwMqVK7F48WKkpaUBANzd3TF58mSMGjVK4mRERET0IrIoJEJDQxEVFYWJEyeiY8eOAICDBw8iICAA6enpCA8PlzghERGR4WTU+S86WQxtODg44LPPPsOgQYN02r///ntMnDhR70tAObRBVDwObcgHhzbkoyyGNuys3UXZT9aDNFH2IyZZTLbMz89HmzZtirS3bt0aBQUFEiQiIiKikpBFITFs2DBER0cXaV++fDmGDBkiQSIiIiLxmPJVG7KYIwE8nWy5c+dOdOjQAQBw+PBhpKenw8/PD1OmTNG+LioqSqqIREREBpHrDbfEIItCIiUlBa1atQLw9C6gAGBvbw97e3ukpKRoX6dQKCTJR0RERMWTRSGxe/duqSMQEREZjVyHJcQgi0KCiIjIlJnyTbtYSBARERmZXG+4JQZZXLVBRERE5RN7JIiIiIyMQxtERERkMFOebMmhDSIiIjIYeySIiIiMzJQnW7KQICIiMjIObRAREVG59OWXX6J+/fqoWrUq2rdvj8TERFH3z0KCiIjIyKS6adf69esxZcoUhIWFISkpCS1atECvXr2QmZkp2mdjIUFERGRkgkibvqKiojB69GiMGDECnp6eWLZsGSwtLfHtt9+W9iNpsZAgIiIqJ/Ly8pCTk6Oz5eXlFfvaJ0+e4NixY/Dx8dG2VapUCT4+Pjh48KBomUxysmXBk+tSRyi1vLw8REREICQkBBYWFlLHqdB4LuSD50JeeD5KTqy/l2bNmoXZs2frtIWFhWHWrFlFXnvnzh0UFhbCyclJp93JyQnnzp0TJQ8AKARTnkpajuXk5MDW1hb379+HjY2N1HEqNJ4L+eC5kBeej7KXl5dXpAfCwsKi2ELuxo0bqF27Ng4cOICOHTtq2z/++GPs3bsXhw8fFiWTSfZIEBERmaIXFQ3Fsbe3R+XKlXHr1i2d9lu3bsHZ2Vm0TJwjQUREZILMzc3RunVrxMfHa9s0Gg3i4+N1eihKiz0SREREJmrKlCnw9/dHmzZt0K5dOyxZsgQPHz7EiBEjRDsGCwmZsrCwQFhYGCcwyQDPhXzwXMgLz4f8/fe//8Xt27cRGhqKjIwMvPrqq9i+fXuRCZilwcmWREREZDDOkSAiIiKDsZAgIiIig7GQICIiIoOxkCCS2J49e6BQKJCdnS11FCJZ429FnlhIEBERkcFYSEigsLAQGo1G6hgEngu54fmQB54H0gcLiRLo1q0bPvroI3z00UewtbWFvb09Zs6cqb03fF5eHoKCglC7dm1YWVmhffv22LNnj/b9MTExqF69OjZv3gxPT09YWFggPT0de/bsQbt27WBlZYXq1aujU6dOuHLlivZ90dHReOWVV2Bubo7GjRvju+++08mlUCjwzTffYMCAAbC0tIS7uzs2b95cos9UWFiIDz74AA0aNIBSqUTjxo2xdOnS0n9ZRmaK5+KZhIQENG/eHFWrVkWHDh2QkpJi+BdVRkz1fJw+fRpvv/02bGxsYG1tjc6dO+PixYul+7KMyFTPw2+//YZGjRpBqVSie/fu+PPPP0v1PZGRCPSvunbtKlSrVk2YNGmScO7cOWHt2rWCpaWlsHz5ckEQBGHUqFGCt7e38McffwgXLlwQPv30U8HCwkI4f/68IAiCsGrVKsHMzEzw9vYWEhIShHPnzgn3798XbG1thaCgIOHChQvCmTNnhJiYGOHKlSuCIAjCpk2bBDMzM+HLL78UUlNThcjISKFy5crC77//rs0FQKhTp44QGxsrpKWlCSqVSqhWrZpw9+7df/1MT548EUJDQ4UjR44Ily5d0n6m9evXG+EbFI8pnovdu3cLAAQPDw9h586dwsmTJ4W3335bqF+/vvDkyRMjfIviMcXzce3aNcHOzk545513hCNHjgipqanCt99+K5w7d84I36A4TPE8pKenCxYWFsKUKVO0n8nJyUkAINy7d0/8L5EMxkKiBLp27Sp4eHgIGo1G2zZ16lTBw8NDuHLlilC5cmXh+vXrOu/p2bOnEBISIgjC0x8pACE5OVn7/N27dwUAwp49e4o9pre3tzB69Gidtvfee0946623tI8BCDNmzNA+VqvVAgBh27ZtBn3OCRMmCO+++65B7y0rpngunhUSP/zwg04mpVJZLgo7UzsfISEhQoMGDWRfxD3PVM+Dp6enTtvUqVNZSMgQhzZKqEOHDlAoFNrHHTt2RFpaGk6dOoXCwkI0atQI1apV02579+7V6Qo1NzdH8+bNtY/t7OwwfPhw9OrVC3379sXSpUtx8+ZN7fNnz55Fp06ddDJ06tQJZ8+e1Wl7fp9WVlawsbFBZmZmiT7Tl19+idatW8PBwQHVqlXD8uXLkZ6eXrIvREKmeC6efY7nMzVu3LjIMeTI1M5HcnIyOnfuDDMzs5J/CTJgaufh7NmzaN++vU6bmDeaIvHwXhulpFarUblyZRw7dgyVK1fWea5atWraPyuVSp0fOQCsWrUKKpUK27dvx/r16zFjxgzExcWhQ4cOJT7+P/9np1AoSjRJ6ocffkBQUBAiIyPRsWNHWFtb49NPPxXt/vRSKK/nwlSV1/OhVCpLfIzyoLyeByo/2CNRQv/8C/bQoUNwd3dHy5YtUVhYiMzMTLi5uelsJbnfe8uWLRESEoIDBw6gWbNmiI2NBQB4eHggISFB57UJCQnw9PQU5fMkJCTA29sb48ePR8uWLeHm5ibryWTPM7Vz8fzneObevXs4f/48PDw8RD2GMZja+WjevDn27duH/Px8UfZXVkztPHh4eCAxMbHIZyL5YY9ECaWnp2PKlCn48MMPkZSUhM8//xyRkZFo1KgRhgwZAj8/P0RGRqJly5a4ffs24uPj0bx5c/Tp06fY/V2+fBnLly+Hr68vXFxckJqairS0NPj5+QEAgoOD8f7776Nly5bw8fHBli1bsGnTJuzatUuUz+Pu7o41a9Zgx44daNCgAb777jscOXIEDRo0EGX/xmRq5+KZ8PBw1KxZE05OTpg+fTrs7e3Rv39/UY9hDKZ2Pj766CN8/vnnGDhwIEJCQmBra4tDhw6hXbt2aNy4sSjHMAZTOw9jx45FZGQkgoODMWrUKBw7dgwxMTGi7JtEJvUkjfKga9euwvjx44WxY8cKNjY2Qo0aNYRPPvlEO7Hp2RUQ9evXF8zMzIRatWoJAwYMEE6ePCkIwtOJTLa2tjr7zMjIEPr37y/UqlVLMDc3F+rVqyeEhoYKhYWF2td89dVXQsOGDQUzMzOhUaNGwpo1a3T2AUD4+eefddpsbW2FVatW/etnevz4sTB8+HDB1tZWqF69ujBu3Dhh2rRpQosWLfT+fsqSKZ6LZ5Mtt2zZIjRt2lQwNzcX2rVrJ5w4cUL/L6iMmeL5EARBOHHihPDGG28IlpaWgrW1tdC5c2fh4sWL+n05ZchUz8OWLVsENzc3wcLCQujcubPw7bffcrKlDPE24iXQrVs3vPrqq1iyZInUUSo8ngt54fmQB54HkhLnSBAREZHBWEiYqLFjx+pc6vX8NnbsWKnjVSg8F/LC8yEPPA+mg0MbJiozMxM5OTnFPmdjYwNHR8cyTlRx8VzIC8+HPPA8mA4WEkRERGQwDm0QERGRwVhIEBERkcFYSBAREZHBWEgQERGRwVhIEElg+PDhOstfd+vWDZMnTy7zHHv27IFCoUB2dnaZH5uITAMLCaK/DB8+HAqFAgqFAubm5nBzc0N4eDgKCgqMfuxNmzZhzpw5JXptWf/lX79+fe33YmVlhVatWmHjxo1lcmwikj8WEkTP6d27N27evIm0tDQEBgZi1qxZ+PTTT4t97ZMnT0Q7rp2dHaytrUXbn9jCw8Nx8+ZNHD9+HG3btsV///tfHDhwwKB9ifm9EZH0WEgQPcfCwgLOzs6oV68exo0bBx8fH2zevBnA38MR8+bNg4uLi/ZOkFevXsX777+P6tWrw87ODv369cOff/6p3WdhYSGmTJmC6tWro2bNmvj444/xz+Vb/jm0kZeXh6lTp8LV1RUWFhZwc3PDypUr8eeff6J79+4AgBo1akChUGD48OEAAI1Gg4iICDRo0ABKpRItWrTAjz/+qHOc3377DY0aNYJSqUT37t11cr6MtbU1nJ2d0ahRI3z55ZdQKpXYsmVLiT7/i763r776Cu7u7qhatSqcnJzwn//8R+fzq1QqODo6omrVqnjttddw5MgR7fPPemXi4+PRpk0bWFpawtvbG6mpqSX6PEQkHhYSRC+hVCp1/gUdHx+P1NRUxMXFYevWrcjPz0evXr1gbW2Nffv2ISEhAdWqVUPv3r2174uMjERMTAy+/fZb7N+/H1lZWfj5559felw/Pz98//33+Oyzz3D27Fl8/fXXqFatGlxdXfHTTz8BAFJTU3Hz5k0sXboUABAREYE1a9Zg2bJlOH36NAICAjB06FDs3bsXwNO/8N955x307dsXycnJGDVqFKZNm6b3d1KlShWYmZnhyZMnJfr8xX1vR48ehUqlQnh4OFJTU7F9+3Z06dJF+/qPP/4YP/30E1avXo2kpCS4ubmhV69eyMrK0skyffp0REZG4ujRo6hSpQpGjhyp9+cholKS8M6jRLLi7+8v9OvXTxAEQdBoNEJcXJxgYWEhBAUFaZ93cnIS8vLytO/57rvvhMaNG2tv1ywIgpCXlycolUphx44dgiAIQq1atYRFixZpn8/Pzxfq1KmjPZYgPL0N9KRJkwRBEITU1FQBgBAXF1dszme3HX/+VsqPHz8WLC0thQMHDui89oMPPhAGDRokCIIghISECJ6enjrPT5069V9vy1yvXj1h8eLF2s82f/58AYCwdevWEn3+4r63n376SbCxsRFycnKKHE+tVgtmZmbCunXrtG1PnjwRXFxctN/js+9g165d2tf8+uuvAgDh0aNHL/wsRCS+KlIWMURys3XrVlSrVg35+fnQaDQYPHgwZs2apX3ey8sL5ubm2scnTpzAhQsXisxvePz4MS5evIj79+/j5s2baN++vfa5KlWqoE2bNkWGN55JTk5G5cqV0bVr1xLnvnDhAnJzc/H666/rtD958gQtW7YEAJw9e1YnBwB07NixRPufOnUqZsyYgcePH6NatWpYsGAB+vTpg+Dg4Jd+/mf++b29/vrrqFevHho2bIjevXujd+/eGDBgACwtLXHx4kXk5+ejU6dO2tebmZmhXbt2OHv2rM5xmjdvrv1zrVq1ADy9h0PdunVL9LmIqPRYSBA9p3v37oiOjoa5uTlcXFxQpYruT8TKykrnsVqtRuvWrbFu3boi+3JwcDAog1Kp1Ps9arUaAPDrr7+idu3aOs9ZWFgYlON5wcHBGD58OKpVqwYnJycoFArtcUvy+f/5vVlbWyMpKQl79uzBzp07ERoailmzZunMgygJMzMz7Z+fZdJoNHrtg4hKh4UE0XOsrKzg5uZW4te3atUK69evh6OjI2xsbIp9Ta1atXD48GHtHICCggIcO3YMrVq1Kvb1Xl5e0Gg02Lt3L3x8fIo8/+xf9oWFhdo2T09PWFhYID09/YU9GR4eHtqJo88cOnTo3z8kAHt7+2K/l5J8/hepUqUKfHx84OPjg7CwMFSvXh2///47evXqBXNzcyQkJKBevXoAgPz8fBw5ckSStTaI6OU42ZKoFIYMGQJ7e3v069cP+/btw+XLl7Fnzx6oVCpcu3YNADBp0iQsWLAAv/zyC86dO4fx48e/dA2I+vXrw9/fHyNHjsQvv/yi3eeGDRsAAPXq1YNCocDWrVtx+/ZtqNVqWFtbIygoCAEBAVi9ejUuXryIpKQkfP7551i9ejUAYOzYsUhLS0NwcDBSU1MRGxuLmJgYo3/+4mzduhWfffYZkpOTceXKFaxZswYajQaNGzeGlZUVxo0bh+DgYGzfvh1nzpzB6NGjkZubiw8++KBUeYlIfCwkiErB0tISf/zxB+rWrYt33nkHHh4e+OCDD/D48WPtv9ADAwMxbNgw+Pv7o2PHjrC2tsaAAQNeut/o6Gj85z//wfjx49GkSROMHj0aDx8+BADUrl0bs2fPxrRp0+Dk5ISPPvoIADBnzhzMnDkTERER8PDwQO/evfHrr7+iQYMGAIC6devip59+wi+//IIWLVpg2bJlmD9/vtE/f3GqV6+OTZs2oUePHvDw8MCyZcvw/fffo2nTpgCABQsW4N1338WwYcPQqlUrXLhwATt27ECNGjVKlZeIxKcQXjTji4iIiOhfsEeCiIiIDMZCgoiIiAzGQoKIiIgMxkKCiIiIDMZCgoiIiAzGQoKIiIgMxkKCiIiIDMZCgoiIiAzGQoKIiIgMxkKCiIiIDMZCgoiIiAz2/8j1B3G2ahQGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions_array = np.array(predictions)\n",
    "true_labels_array = np.array(test_batch[1])\n",
    "\n",
    "predicted_classes = np.argmax(predictions_array, axis=1)\n",
    "true_classes = np.argmax(true_labels_array, axis=1)\n",
    "\n",
    "predicted_labels = np.array(class_labels)[predicted_classes]\n",
    "true_labels = np.array(class_labels)[true_classes]\n",
    "\n",
    "\n",
    "cm = confusion_matrix(predicted_labels, true_labels, labels = class_labels)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "# Add labels to the axes\n",
    "plt.xlabel('Predicted Person')\n",
    "plt.ylabel('True Person')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
